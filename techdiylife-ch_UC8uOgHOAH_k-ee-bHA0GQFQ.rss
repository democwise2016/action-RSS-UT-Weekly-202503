<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:media="http://www.rssboard.org/media-rss" version="2.0">
  <channel>
    <title><![CDATA[AI开发者-就爱瞎鼓捣[YT+]]]></title>
    <link>http://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ</link>
    <image>
      <url>https://yt3.googleusercontent.com/IE_8MkhRZYH8w0c-SIBy7gqPyXdn7GMrw9fIUMppAsjit-Smx29Zl-simURoP4avMjcvwd6xzQ=s900-b50-c-k-c0x008A95A5-no-rj</url>
      <title>AI开发者-就爱瞎鼓捣[YT+]</title>
      <link>http://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ</link>
    </image>
    <language>en-us</language>
    <atom:link href="https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ" rel="self" type="application/rss+xml"/>
    <copyright><![CDATA[AI开发者-就爱瞎鼓捣[YT+]]]></copyright>
    <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣[YT+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ" target="_blank">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ</a>
      ]]>
    </itunes:summary>
    <description>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ" target="_blank">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ</a>
      ]]>
    </description>
    <itunes:owner>
      <itunes:name><![CDATA[AI开发者-就爱瞎鼓捣[YT+]]]></itunes:name>
    </itunes:owner>
    <itunes:image href="https://yt3.googleusercontent.com/IE_8MkhRZYH8w0c-SIBy7gqPyXdn7GMrw9fIUMppAsjit-Smx29Zl-simURoP4avMjcvwd6xzQ=s900-b50-c-k-c0x008A95A5-no-rj"/>
<item>
      <title><![CDATA[Gradio一行代码搞定了语音交互前端]]></title>
      <link>https://www.youtube.com/watch?v=TJPVpMyyvRc</link>
      <itunes:title><![CDATA[Gradio一行代码搞定了语音交互前端]]></itunes:title>
      <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/TJPVpMyyvRc/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=TJPVpMyyvRc">https://www.youtube.com/watch?v=TJPVpMyyvRc</a></p><h1>值得閱讀的理由</h1> <ul> <li>了解如何利用Gradio大幅減少開發基於Web的語音識別/合成應用所需的程式碼量。</li> <li>學習Gradio如何簡化前端複雜性，讓專注於後端模型開發的工程師也能快速打造美觀實用的Demo。</li> <li>探索Gradio `Audio` 組件的強大功能，包括錄音、播放、串流處理及即時語音生成等。</li> </ul> <hr /> <h1>摘要</h1> <p>作者首先提出一個問題：若要為Whisper語音識別模型建立一個基於Web的Demo，需要多少程式碼？他透過Claude生成的例子指出，前端部分（HTML、JavaScript用於語音收集、CSS樣式）通常需要數百行程式碼（如654行，其中CSS佔330行），而後端進行語音識別處理的核心邏輯可能僅需數十行（如56行）。這凸顯了前端開發的複雜性和所需程式碼量遠超後端模型邏輯的現狀。作者提到，對於專注於後端模型性能開發的人來說，前端工作往往是短板，即使模型性能大幅提升，粗糙的Demo也可能無法給人留下深刻印象。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_50.jpg" /></p> <h2>Gradio：簡化Demo開發的利器</h2> <p>為了解決上述問題，作者引入了Huggingface開發的<strong>Gradio</strong>工具。Gradio能夠將同樣的Web語音Demo程式碼量大幅縮減至約100行，且這100行程式碼同時包含了前端和後端邏輯。其核心在於Gradio對前端介面和信號處理部分進行了高度封裝，特別是透過其<strong>Audio組件</strong>。這個強大的組件不僅支援語音檔案上傳、麥克風數據讀取、簡單編輯和播放，還能方便地在<strong>串流（streaming）</strong>和非串流模式之間切換，極大地簡化了開發過程。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_120.jpg" /></p> <h2>Gradio實例：語音識別與語音合成</h2> <p>作者接著展示了兩個Gradio實例。第一個是支援檔案上傳和麥克風輸入的語音識別程式。第二個是一個多角色語音合成小說的演示，使用了Cosy Voice 2.0，生動地展示了Gradio實現<strong>一邊生成一邊播放</strong>的複雜功能，其中包含多個語氣和角色之間的對話，展現了Gradio在處理即時語音生成方面的強大能力。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_340.jpg" /></p> <h2>Gradio實現細節：語音識別</h2> <p>作者深入探討了Gradio實現語音識別的細節。他先展示了一個基於Whisper模型的<strong>非串流處理</strong>範例，說明了如何使用<code>gr.Interface</code>組件，透過簡單地指定處理函數、<code>gr.Audio</code>作為麥克風輸入，以及<code>gr.Textbox</code>作為輸出，就能快速搭建Demo。隨後，他介紹了實現<strong>串流處理</strong>的範例，指出主要有三個變化：將<code>Audio</code>組件的<code>streaming</code>參數設為<code>True</code>、增加<code>gr.State</code>組件用於保存已收集數據，以及修改處理函數以適應即時數據。此外，作者也提到了除了高封裝度的<code>gr.Interface</code>，也可以使用<code>gr.Blocks</code>組件，透過<code>Audio.stream</code>事件來綁定函數與輸入輸出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_7.jpg" /></p> <h2>Gradio實現細節：語音合成</h2> <p>在語音合成方面，作者以之前展示的「一邊合成一邊輸出」功能為例。要實現這個功能，同樣需要將<code>Audio</code>組件的<code>streaming</code>參數設為<code>True</code>，並將<code>autoplay</code>設為<code>True</code>以便自動播放。語音生成邏輯會根據角色和語氣設定逐句合成語音，並將合併後的<code>.wav</code>檔案路徑傳送給<code>Audio</code>組件。當<code>Audio</code>組件接收到路徑更新時，它會自動刷新語音檔案並播放，且不影響當前正在進行的播放。作者補充說明，他使用了語音檔案傳遞的方式，但使用NumPy數據格式也應該可行。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_9.jpg" /></p> <h2>結語：Gradio的價值與應用</h2> <p>總結來說，作者強調Gradio提供的<strong>Audio組件</strong>功能極其強大，幾乎能覆蓋大部分語音處理需求。儘管其預設介面並非「高度美顏版」，但也足以稱得上「素顏當中長不錯的了」。對於那些專注於模型性能開發，卻不擅長前端設計的工程師而言，Gradio無疑是一個極佳的工具，能幫助他們快速製作出美觀且功能完善的Demo。作者鼓勵觀眾親自嘗試使用Gradio來製作語音識別和語音合成應用。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">各位小伙伴大家好。今天呢我们来看一下。如何制作一个语音识别的Demo。大家可以先来思考一个问题啊。假如给你一个whisper的语音识别模型。让你制造一个基于Web的。语音识别的Demo。你觉得需要多少代码呢。100行 500行。还是1,000行呢。那么再进一步问一下。你觉得是前端的代码多。还是后端。进行语音识别处理的代码多呢。这边呢是Claude模型帮我写的一个代码。前端呢是通过html构建的界面。通过Javascript收集语音。代码呢一共是654行。后端呢它通过Flask。启动了一个基于Qwen2-Audio的模型。来进行语音识别的处理。代码呢一共是56行。再来看一下这个外部前端的。代码部分其中CSS样式的部分呢。是330行。Javascript的语音收集处理部分240行。界面组件的部分呢。是66行那即便是不需要这个CSS。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">进行界面的这个化妆。那也需要至少300行的代码。来实现这个功能。而后端的代码我们可以看一下。就只有56行。那对于一款产品来说呢。前端的美观和交互性呢。可以说是非常重要的。但是对于一个专注后端模型。性能开发的人来说。他可能啊。非常不擅长前端这些工作。做出来的demo呢。可能是这样的或者呢。是这样的。费了吃奶的力气。让模型的性能啊大幅度改善了。给老板展示了一下。但是老板看了之后呢。却觉得啥也不是。那如何让专注改善大模型性能的人。快速的搞出一个还算漂亮的demo呢。这就要提到Huggingface开发的这个Gradio了。我们可以来看看Gradio。制作一个同样的Demo。它需要多少行代码呢。答案是只有100行代码。这100行代码呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时包含了前端和后端。其中语音识别处理的部分呢。与前面的代码并没有什么区别。区别之处呢。就是Gradio对这个前端的界面。和信号处理的部分啊。进行了高度的封装。也就是呢。将这些部分啊。都封装进了这个Audio这个组件当中。通过这个组件。你可以完成语音文件的上传。从麦克风呢读取数据。甚至呢。还支持对语音文件的简单编辑。不仅如此。它还可以对语音呢进行播放。同时呢还可以非常方便的。在streaming和NON streaming之间啊。来回的切换。那首先呢我们先看两个例子。这个呢。是我制作的一个语音识别的程序。它同时呢可以支持文件上传。麦克风的输入。再来看一个语音合成的例子。这是呢。一个多角色语音合成小说的一个演示。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这位同学快醒醒。开始上课学习Gradio制作演示程序了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_158.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次我们来测试一款TTS工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是cosy voice 2.0。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">下面一起来看看它的效果如何。你可拉倒吧。学那么多有啥用。天天学这个学那个的有个毛用啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还不如刷会短视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这人是谁啊。是不是跑错教室了。在这里瞎掰掰。传递负能量。赶紧赶出去哈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你是哪里来的野丫头。活腻歪了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">竟然挑战万人敬仰的本太乙真人。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">小心我一剑劈死你。哟这么热闹啊。大名鼎鼎的太乙真人。劈死一个小姑娘。真是有本事呵呵。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你又是谁啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">别多管闲事。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再在这里瞎BB。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">连你一起也劈了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有你正在看演示的这位玉树临风。英俊不凡。风度翩翩的大帅哥。快给我点赞。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一键三连一。会给你表演一剑劈人。你可以看到。它这里实现的是一边生成一边播放的。一个看起来挺复杂的一个功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些功能。它到底都是如何来进行实现的呢。首先我们可以看一下这个官方的例子。这个代码呢。是一个非streaming处理的一个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢。它使用的模型是一个whisper的模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后这个函数呢。就是用来这个接受语音之后。进行语音识别处理的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最下面的部分呢是这个INTERFACE的组件。通过这个组件。可以让你啊快速的实现一些demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先这里是指定前面的函数。然后增加了一个Audio的组件。通过这个参数。指定是通过麦克风来作为输入的。最后一个参数呢。就是用来指定。用这个text的组件来作为输出。怎么样从代码的数量上来看。是不是觉得非常的简单呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后咱们再来看一下它的第二个例子。这个呢是一个streaming处理的一个例子。这个例子与前面相比啊。主要有三个变化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是将Audio的streaming参数啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">设置为了true。第2点呢就是增加了一个state的组件。这个组件啊。它是用来保存。这个前面已经收录的数据的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是对这个Transcript这个函数啊。进行了一些修改。从而让它。可以啊来处理这种实时收集的数据。那这个呢。它是一个基于INTERFACE组件的一个实现。这种方式呢它的封装度啊是比较高的。这边呢是我使用这个blocks组件。重新实现了一个类似的功能。主要的一个区别呢。这边就是通过这个Audio的stream事件。来将这个函数和输入输出呢。进行的绑定好。最后呢我们再来看一下。如何实现这个语音合成。例子当中的一个一边合成。一边进行输出的一个功能。这里同样的。要对Audio组件的streaming参数呢。设置为true。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_315.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢因为啊我们需要一边进行合成。一边呢播放它的语音。所以呢。需要进一步设置这个auto play为true。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样呢它就可以自动啊进行播放了。关于语音生成呢。这边设计的呢。会根据前面这个角色。以及这里的语气的设置。来合成相关的语音。这边呢就是将逐句合成的语音呢。合并成了一个wav文件。然后呢。将这个路径呢发送给这个Audio组件。Audio组件呢。每次接收到这个路径的更新呢。会自动刷新这个语音文件。并且呢还不影响它现在进行的播放。我这个代码呢。使用的是通过语音文件传递的方式。你如果设置成。numpy的这个数据格式的话。应该也是可以的。感兴趣的小伙伴呢可以自己来试一下。通过上面这两个例子啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们可以看到。Gradio提供的这个Audio的组件。可以说是功能的非常的强大。它差不多啊可以覆盖掉我们语音处理。的很多的功能了。并且它的默认界面呢。看起来还是挺不错的。虽然不是高度的美颜版。但是呢也是素颜当中啊长不错的了。对于专注模型。性能开发的小伙伴来说呢。这个Demo。我个人觉得还是非常的不错的。那你觉得怎么样呢。好以上就是关于我们使用这个Gradio。来制作语音识别和语音合成的例子。怎么样看起来还不错吧。赶紧去试试吧。好以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>Gradio一行代码搞定了语音交互前端</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/TJPVpMyyvRc/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=TJPVpMyyvRc">https://www.youtube.com/watch?v=TJPVpMyyvRc</a></p><h1>值得閱讀的理由</h1> <ul> <li>了解如何利用Gradio大幅減少開發基於Web的語音識別/合成應用所需的程式碼量。</li> <li>學習Gradio如何簡化前端複雜性，讓專注於後端模型開發的工程師也能快速打造美觀實用的Demo。</li> <li>探索Gradio `Audio` 組件的強大功能，包括錄音、播放、串流處理及即時語音生成等。</li> </ul> <hr /> <h1>摘要</h1> <p>作者首先提出一個問題：若要為Whisper語音識別模型建立一個基於Web的Demo，需要多少程式碼？他透過Claude生成的例子指出，前端部分（HTML、JavaScript用於語音收集、CSS樣式）通常需要數百行程式碼（如654行，其中CSS佔330行），而後端進行語音識別處理的核心邏輯可能僅需數十行（如56行）。這凸顯了前端開發的複雜性和所需程式碼量遠超後端模型邏輯的現狀。作者提到，對於專注於後端模型性能開發的人來說，前端工作往往是短板，即使模型性能大幅提升，粗糙的Demo也可能無法給人留下深刻印象。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_50.jpg" /></p> <h2>Gradio：簡化Demo開發的利器</h2> <p>為了解決上述問題，作者引入了Huggingface開發的<strong>Gradio</strong>工具。Gradio能夠將同樣的Web語音Demo程式碼量大幅縮減至約100行，且這100行程式碼同時包含了前端和後端邏輯。其核心在於Gradio對前端介面和信號處理部分進行了高度封裝，特別是透過其<strong>Audio組件</strong>。這個強大的組件不僅支援語音檔案上傳、麥克風數據讀取、簡單編輯和播放，還能方便地在<strong>串流（streaming）</strong>和非串流模式之間切換，極大地簡化了開發過程。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_120.jpg" /></p> <h2>Gradio實例：語音識別與語音合成</h2> <p>作者接著展示了兩個Gradio實例。第一個是支援檔案上傳和麥克風輸入的語音識別程式。第二個是一個多角色語音合成小說的演示，使用了Cosy Voice 2.0，生動地展示了Gradio實現<strong>一邊生成一邊播放</strong>的複雜功能，其中包含多個語氣和角色之間的對話，展現了Gradio在處理即時語音生成方面的強大能力。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_340.jpg" /></p> <h2>Gradio實現細節：語音識別</h2> <p>作者深入探討了Gradio實現語音識別的細節。他先展示了一個基於Whisper模型的<strong>非串流處理</strong>範例，說明了如何使用<code>gr.Interface</code>組件，透過簡單地指定處理函數、<code>gr.Audio</code>作為麥克風輸入，以及<code>gr.Textbox</code>作為輸出，就能快速搭建Demo。隨後，他介紹了實現<strong>串流處理</strong>的範例，指出主要有三個變化：將<code>Audio</code>組件的<code>streaming</code>參數設為<code>True</code>、增加<code>gr.State</code>組件用於保存已收集數據，以及修改處理函數以適應即時數據。此外，作者也提到了除了高封裝度的<code>gr.Interface</code>，也可以使用<code>gr.Blocks</code>組件，透過<code>Audio.stream</code>事件來綁定函數與輸入輸出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_7.jpg" /></p> <h2>Gradio實現細節：語音合成</h2> <p>在語音合成方面，作者以之前展示的「一邊合成一邊輸出」功能為例。要實現這個功能，同樣需要將<code>Audio</code>組件的<code>streaming</code>參數設為<code>True</code>，並將<code>autoplay</code>設為<code>True</code>以便自動播放。語音生成邏輯會根據角色和語氣設定逐句合成語音，並將合併後的<code>.wav</code>檔案路徑傳送給<code>Audio</code>組件。當<code>Audio</code>組件接收到路徑更新時，它會自動刷新語音檔案並播放，且不影響當前正在進行的播放。作者補充說明，他使用了語音檔案傳遞的方式，但使用NumPy數據格式也應該可行。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_9.jpg" /></p> <h2>結語：Gradio的價值與應用</h2> <p>總結來說，作者強調Gradio提供的<strong>Audio組件</strong>功能極其強大，幾乎能覆蓋大部分語音處理需求。儘管其預設介面並非「高度美顏版」，但也足以稱得上「素顏當中長不錯的了」。對於那些專注於模型性能開發，卻不擅長前端設計的工程師而言，Gradio無疑是一個極佳的工具，能幫助他們快速製作出美觀且功能完善的Demo。作者鼓勵觀眾親自嘗試使用Gradio來製作語音識別和語音合成應用。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">各位小伙伴大家好。今天呢我们来看一下。如何制作一个语音识别的Demo。大家可以先来思考一个问题啊。假如给你一个whisper的语音识别模型。让你制造一个基于Web的。语音识别的Demo。你觉得需要多少代码呢。100行 500行。还是1,000行呢。那么再进一步问一下。你觉得是前端的代码多。还是后端。进行语音识别处理的代码多呢。这边呢是Claude模型帮我写的一个代码。前端呢是通过html构建的界面。通过Javascript收集语音。代码呢一共是654行。后端呢它通过Flask。启动了一个基于Qwen2-Audio的模型。来进行语音识别的处理。代码呢一共是56行。再来看一下这个外部前端的。代码部分其中CSS样式的部分呢。是330行。Javascript的语音收集处理部分240行。界面组件的部分呢。是66行那即便是不需要这个CSS。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">进行界面的这个化妆。那也需要至少300行的代码。来实现这个功能。而后端的代码我们可以看一下。就只有56行。那对于一款产品来说呢。前端的美观和交互性呢。可以说是非常重要的。但是对于一个专注后端模型。性能开发的人来说。他可能啊。非常不擅长前端这些工作。做出来的demo呢。可能是这样的或者呢。是这样的。费了吃奶的力气。让模型的性能啊大幅度改善了。给老板展示了一下。但是老板看了之后呢。却觉得啥也不是。那如何让专注改善大模型性能的人。快速的搞出一个还算漂亮的demo呢。这就要提到Huggingface开发的这个Gradio了。我们可以来看看Gradio。制作一个同样的Demo。它需要多少行代码呢。答案是只有100行代码。这100行代码呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时包含了前端和后端。其中语音识别处理的部分呢。与前面的代码并没有什么区别。区别之处呢。就是Gradio对这个前端的界面。和信号处理的部分啊。进行了高度的封装。也就是呢。将这些部分啊。都封装进了这个Audio这个组件当中。通过这个组件。你可以完成语音文件的上传。从麦克风呢读取数据。甚至呢。还支持对语音文件的简单编辑。不仅如此。它还可以对语音呢进行播放。同时呢还可以非常方便的。在streaming和NON streaming之间啊。来回的切换。那首先呢我们先看两个例子。这个呢。是我制作的一个语音识别的程序。它同时呢可以支持文件上传。麦克风的输入。再来看一个语音合成的例子。这是呢。一个多角色语音合成小说的一个演示。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这位同学快醒醒。开始上课学习Gradio制作演示程序了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_158.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次我们来测试一款TTS工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是cosy voice 2.0。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">下面一起来看看它的效果如何。你可拉倒吧。学那么多有啥用。天天学这个学那个的有个毛用啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还不如刷会短视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这人是谁啊。是不是跑错教室了。在这里瞎掰掰。传递负能量。赶紧赶出去哈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你是哪里来的野丫头。活腻歪了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">竟然挑战万人敬仰的本太乙真人。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">小心我一剑劈死你。哟这么热闹啊。大名鼎鼎的太乙真人。劈死一个小姑娘。真是有本事呵呵。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你又是谁啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">别多管闲事。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再在这里瞎BB。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">连你一起也劈了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有你正在看演示的这位玉树临风。英俊不凡。风度翩翩的大帅哥。快给我点赞。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一键三连一。会给你表演一剑劈人。你可以看到。它这里实现的是一边生成一边播放的。一个看起来挺复杂的一个功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些功能。它到底都是如何来进行实现的呢。首先我们可以看一下这个官方的例子。这个代码呢。是一个非streaming处理的一个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢。它使用的模型是一个whisper的模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后这个函数呢。就是用来这个接受语音之后。进行语音识别处理的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最下面的部分呢是这个INTERFACE的组件。通过这个组件。可以让你啊快速的实现一些demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先这里是指定前面的函数。然后增加了一个Audio的组件。通过这个参数。指定是通过麦克风来作为输入的。最后一个参数呢。就是用来指定。用这个text的组件来作为输出。怎么样从代码的数量上来看。是不是觉得非常的简单呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后咱们再来看一下它的第二个例子。这个呢是一个streaming处理的一个例子。这个例子与前面相比啊。主要有三个变化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是将Audio的streaming参数啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">设置为了true。第2点呢就是增加了一个state的组件。这个组件啊。它是用来保存。这个前面已经收录的数据的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是对这个Transcript这个函数啊。进行了一些修改。从而让它。可以啊来处理这种实时收集的数据。那这个呢。它是一个基于INTERFACE组件的一个实现。这种方式呢它的封装度啊是比较高的。这边呢是我使用这个blocks组件。重新实现了一个类似的功能。主要的一个区别呢。这边就是通过这个Audio的stream事件。来将这个函数和输入输出呢。进行的绑定好。最后呢我们再来看一下。如何实现这个语音合成。例子当中的一个一边合成。一边进行输出的一个功能。这里同样的。要对Audio组件的streaming参数呢。设置为true。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_315.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢因为啊我们需要一边进行合成。一边呢播放它的语音。所以呢。需要进一步设置这个auto play为true。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样呢它就可以自动啊进行播放了。关于语音生成呢。这边设计的呢。会根据前面这个角色。以及这里的语气的设置。来合成相关的语音。这边呢就是将逐句合成的语音呢。合并成了一个wav文件。然后呢。将这个路径呢发送给这个Audio组件。Audio组件呢。每次接收到这个路径的更新呢。会自动刷新这个语音文件。并且呢还不影响它现在进行的播放。我这个代码呢。使用的是通过语音文件传递的方式。你如果设置成。numpy的这个数据格式的话。应该也是可以的。感兴趣的小伙伴呢可以自己来试一下。通过上面这两个例子啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们可以看到。Gradio提供的这个Audio的组件。可以说是功能的非常的强大。它差不多啊可以覆盖掉我们语音处理。的很多的功能了。并且它的默认界面呢。看起来还是挺不错的。虽然不是高度的美颜版。但是呢也是素颜当中啊长不错的了。对于专注模型。性能开发的小伙伴来说呢。这个Demo。我个人觉得还是非常的不错的。那你觉得怎么样呢。好以上就是关于我们使用这个Gradio。来制作语音识别和语音合成的例子。怎么样看起来还不错吧。赶紧去试试吧。好以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>Gradio一行代码搞定了语音交互前端</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/TJPVpMyyvRc/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=TJPVpMyyvRc">https://www.youtube.com/watch?v=TJPVpMyyvRc</a></p><h1>值得閱讀的理由</h1> <ul> <li>了解如何利用Gradio大幅減少開發基於Web的語音識別/合成應用所需的程式碼量。</li> <li>學習Gradio如何簡化前端複雜性，讓專注於後端模型開發的工程師也能快速打造美觀實用的Demo。</li> <li>探索Gradio `Audio` 組件的強大功能，包括錄音、播放、串流處理及即時語音生成等。</li> </ul> <hr /> <h1>摘要</h1> <p>作者首先提出一個問題：若要為Whisper語音識別模型建立一個基於Web的Demo，需要多少程式碼？他透過Claude生成的例子指出，前端部分（HTML、JavaScript用於語音收集、CSS樣式）通常需要數百行程式碼（如654行，其中CSS佔330行），而後端進行語音識別處理的核心邏輯可能僅需數十行（如56行）。這凸顯了前端開發的複雜性和所需程式碼量遠超後端模型邏輯的現狀。作者提到，對於專注於後端模型性能開發的人來說，前端工作往往是短板，即使模型性能大幅提升，粗糙的Demo也可能無法給人留下深刻印象。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_50.jpg" /></p> <h2>Gradio：簡化Demo開發的利器</h2> <p>為了解決上述問題，作者引入了Huggingface開發的<strong>Gradio</strong>工具。Gradio能夠將同樣的Web語音Demo程式碼量大幅縮減至約100行，且這100行程式碼同時包含了前端和後端邏輯。其核心在於Gradio對前端介面和信號處理部分進行了高度封裝，特別是透過其<strong>Audio組件</strong>。這個強大的組件不僅支援語音檔案上傳、麥克風數據讀取、簡單編輯和播放，還能方便地在<strong>串流（streaming）</strong>和非串流模式之間切換，極大地簡化了開發過程。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_120.jpg" /></p> <h2>Gradio實例：語音識別與語音合成</h2> <p>作者接著展示了兩個Gradio實例。第一個是支援檔案上傳和麥克風輸入的語音識別程式。第二個是一個多角色語音合成小說的演示，使用了Cosy Voice 2.0，生動地展示了Gradio實現<strong>一邊生成一邊播放</strong>的複雜功能，其中包含多個語氣和角色之間的對話，展現了Gradio在處理即時語音生成方面的強大能力。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_340.jpg" /></p> <h2>Gradio實現細節：語音識別</h2> <p>作者深入探討了Gradio實現語音識別的細節。他先展示了一個基於Whisper模型的<strong>非串流處理</strong>範例，說明了如何使用<code>gr.Interface</code>組件，透過簡單地指定處理函數、<code>gr.Audio</code>作為麥克風輸入，以及<code>gr.Textbox</code>作為輸出，就能快速搭建Demo。隨後，他介紹了實現<strong>串流處理</strong>的範例，指出主要有三個變化：將<code>Audio</code>組件的<code>streaming</code>參數設為<code>True</code>、增加<code>gr.State</code>組件用於保存已收集數據，以及修改處理函數以適應即時數據。此外，作者也提到了除了高封裝度的<code>gr.Interface</code>，也可以使用<code>gr.Blocks</code>組件，透過<code>Audio.stream</code>事件來綁定函數與輸入輸出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_7.jpg" /></p> <h2>Gradio實現細節：語音合成</h2> <p>在語音合成方面，作者以之前展示的「一邊合成一邊輸出」功能為例。要實現這個功能，同樣需要將<code>Audio</code>組件的<code>streaming</code>參數設為<code>True</code>，並將<code>autoplay</code>設為<code>True</code>以便自動播放。語音生成邏輯會根據角色和語氣設定逐句合成語音，並將合併後的<code>.wav</code>檔案路徑傳送給<code>Audio</code>組件。當<code>Audio</code>組件接收到路徑更新時，它會自動刷新語音檔案並播放，且不影響當前正在進行的播放。作者補充說明，他使用了語音檔案傳遞的方式，但使用NumPy數據格式也應該可行。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_9.jpg" /></p> <h2>結語：Gradio的價值與應用</h2> <p>總結來說，作者強調Gradio提供的<strong>Audio組件</strong>功能極其強大，幾乎能覆蓋大部分語音處理需求。儘管其預設介面並非「高度美顏版」，但也足以稱得上「素顏當中長不錯的了」。對於那些專注於模型性能開發，卻不擅長前端設計的工程師而言，Gradio無疑是一個極佳的工具，能幫助他們快速製作出美觀且功能完善的Demo。作者鼓勵觀眾親自嘗試使用Gradio來製作語音識別和語音合成應用。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">各位小伙伴大家好。今天呢我们来看一下。如何制作一个语音识别的Demo。大家可以先来思考一个问题啊。假如给你一个whisper的语音识别模型。让你制造一个基于Web的。语音识别的Demo。你觉得需要多少代码呢。100行 500行。还是1,000行呢。那么再进一步问一下。你觉得是前端的代码多。还是后端。进行语音识别处理的代码多呢。这边呢是Claude模型帮我写的一个代码。前端呢是通过html构建的界面。通过Javascript收集语音。代码呢一共是654行。后端呢它通过Flask。启动了一个基于Qwen2-Audio的模型。来进行语音识别的处理。代码呢一共是56行。再来看一下这个外部前端的。代码部分其中CSS样式的部分呢。是330行。Javascript的语音收集处理部分240行。界面组件的部分呢。是66行那即便是不需要这个CSS。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">进行界面的这个化妆。那也需要至少300行的代码。来实现这个功能。而后端的代码我们可以看一下。就只有56行。那对于一款产品来说呢。前端的美观和交互性呢。可以说是非常重要的。但是对于一个专注后端模型。性能开发的人来说。他可能啊。非常不擅长前端这些工作。做出来的demo呢。可能是这样的或者呢。是这样的。费了吃奶的力气。让模型的性能啊大幅度改善了。给老板展示了一下。但是老板看了之后呢。却觉得啥也不是。那如何让专注改善大模型性能的人。快速的搞出一个还算漂亮的demo呢。这就要提到Huggingface开发的这个Gradio了。我们可以来看看Gradio。制作一个同样的Demo。它需要多少行代码呢。答案是只有100行代码。这100行代码呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时包含了前端和后端。其中语音识别处理的部分呢。与前面的代码并没有什么区别。区别之处呢。就是Gradio对这个前端的界面。和信号处理的部分啊。进行了高度的封装。也就是呢。将这些部分啊。都封装进了这个Audio这个组件当中。通过这个组件。你可以完成语音文件的上传。从麦克风呢读取数据。甚至呢。还支持对语音文件的简单编辑。不仅如此。它还可以对语音呢进行播放。同时呢还可以非常方便的。在streaming和NON streaming之间啊。来回的切换。那首先呢我们先看两个例子。这个呢。是我制作的一个语音识别的程序。它同时呢可以支持文件上传。麦克风的输入。再来看一个语音合成的例子。这是呢。一个多角色语音合成小说的一个演示。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这位同学快醒醒。开始上课学习Gradio制作演示程序了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_158.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次我们来测试一款TTS工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是cosy voice 2.0。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">下面一起来看看它的效果如何。你可拉倒吧。学那么多有啥用。天天学这个学那个的有个毛用啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还不如刷会短视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这人是谁啊。是不是跑错教室了。在这里瞎掰掰。传递负能量。赶紧赶出去哈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你是哪里来的野丫头。活腻歪了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">竟然挑战万人敬仰的本太乙真人。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">小心我一剑劈死你。哟这么热闹啊。大名鼎鼎的太乙真人。劈死一个小姑娘。真是有本事呵呵。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你又是谁啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">别多管闲事。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再在这里瞎BB。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">连你一起也劈了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有你正在看演示的这位玉树临风。英俊不凡。风度翩翩的大帅哥。快给我点赞。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一键三连一。会给你表演一剑劈人。你可以看到。它这里实现的是一边生成一边播放的。一个看起来挺复杂的一个功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些功能。它到底都是如何来进行实现的呢。首先我们可以看一下这个官方的例子。这个代码呢。是一个非streaming处理的一个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢。它使用的模型是一个whisper的模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后这个函数呢。就是用来这个接受语音之后。进行语音识别处理的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最下面的部分呢是这个INTERFACE的组件。通过这个组件。可以让你啊快速的实现一些demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先这里是指定前面的函数。然后增加了一个Audio的组件。通过这个参数。指定是通过麦克风来作为输入的。最后一个参数呢。就是用来指定。用这个text的组件来作为输出。怎么样从代码的数量上来看。是不是觉得非常的简单呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后咱们再来看一下它的第二个例子。这个呢是一个streaming处理的一个例子。这个例子与前面相比啊。主要有三个变化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是将Audio的streaming参数啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">设置为了true。第2点呢就是增加了一个state的组件。这个组件啊。它是用来保存。这个前面已经收录的数据的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是对这个Transcript这个函数啊。进行了一些修改。从而让它。可以啊来处理这种实时收集的数据。那这个呢。它是一个基于INTERFACE组件的一个实现。这种方式呢它的封装度啊是比较高的。这边呢是我使用这个blocks组件。重新实现了一个类似的功能。主要的一个区别呢。这边就是通过这个Audio的stream事件。来将这个函数和输入输出呢。进行的绑定好。最后呢我们再来看一下。如何实现这个语音合成。例子当中的一个一边合成。一边进行输出的一个功能。这里同样的。要对Audio组件的streaming参数呢。设置为true。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/TJPVpMyyvRc_315.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢因为啊我们需要一边进行合成。一边呢播放它的语音。所以呢。需要进一步设置这个auto play为true。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样呢它就可以自动啊进行播放了。关于语音生成呢。这边设计的呢。会根据前面这个角色。以及这里的语气的设置。来合成相关的语音。这边呢就是将逐句合成的语音呢。合并成了一个wav文件。然后呢。将这个路径呢发送给这个Audio组件。Audio组件呢。每次接收到这个路径的更新呢。会自动刷新这个语音文件。并且呢还不影响它现在进行的播放。我这个代码呢。使用的是通过语音文件传递的方式。你如果设置成。numpy的这个数据格式的话。应该也是可以的。感兴趣的小伙伴呢可以自己来试一下。通过上面这两个例子啊。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们可以看到。Gradio提供的这个Audio的组件。可以说是功能的非常的强大。它差不多啊可以覆盖掉我们语音处理。的很多的功能了。并且它的默认界面呢。看起来还是挺不错的。虽然不是高度的美颜版。但是呢也是素颜当中啊长不错的了。对于专注模型。性能开发的小伙伴来说呢。这个Demo。我个人觉得还是非常的不错的。那你觉得怎么样呢。好以上就是关于我们使用这个Gradio。来制作语音识别和语音合成的例子。怎么样看起来还不错吧。赶紧去试试吧。好以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>Gradio一行代码搞定了语音交互前端</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/TJPVpMyyvRc/hqdefault.jpg"/>
      <pubDate>2025-08-31T08:33:35.000Z</pubDate>
    </item><item>
      <title><![CDATA[用Gradio+GPT-OSS做了个翻译器，效果吊打谷歌？]]></title>
      <link>https://www.youtube.com/watch?v=Odvl5jeFNJk</link>
      <itunes:title><![CDATA[用Gradio+GPT-OSS做了个翻译器，效果吊打谷歌？]]></itunes:title>
      <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/Odvl5jeFNJk/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=Odvl5jeFNJk">https://www.youtube.com/watch?v=Odvl5jeFNJk</a></p><h1>值得閱讀的理由</h1><ul><li>了解如何使用Gradio輕鬆構建基於大型模型的互動式AI演示程式。</li><li>深入理解OpenAI GPT-OSS 20B模型在文本翻譯應用中的實踐，特別是串流處理的實現方式。</li><li>掌握作者關於語音識別、語音合成及多角色語音生成等未來AI應用開發的詳細計畫。</li></ul><hr /><h1>摘要</h1><h2>引言：Gradio與GPT-OSS 20B的創新結合</h2><p>作者近日展示了一個模仿Google翻譯的小型演示程式，其背後採用了OpenAI最新發布的GPT-OSS 20B大模型。這個程式透過Gradio開發，核心程式碼不超過十行，且能一鍵分享為方便大眾訪問和使用的Web服務。作者預告，該翻譯demo將在本影片發布後一天內公開，並會提供完整的教學和程式碼。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_25.jpg" /></p><p>這個演示程式是作者頻道「共學題目」的一部分，旨在探討如何使用Gradio製作基於大模型的演示程式。作者指出，雖然目前大語言模型多基於Python開發，但Python程式要製作可分享的Demo面臨多重挑戰。</p><h2>AI模型演示的挑戰與Gradio的解決方案</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_55.jpg" /></p><p>這些挑戰包括：首先，使用者需要安裝Python運行環境；其次，當前大模型對硬體要求高，例如GPT-OSS模型至少需要16G顯存的GPU；最後，Python預設的命令列交互方式對一般用戶來說極不友善。為了解決這些問題，作者引入了<strong>Gradio</strong>，它能讓開發者輕鬆構建<strong>交互式AI演示程式</strong>，並方便地分享出去。</p><h2>Gradio學習與未來演示計畫</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_115.jpg" /></p><p>本次共學內容規劃包含學習Gradio的<strong>基本知識</strong>，並運用其製作多種小型Demo。目前，作者計畫製作基於文本處理的翻譯Demo（即影片中介紹的翻譯器），此外還將陸續開發<strong>語音識別</strong>和<strong>語音合成</strong>Demo，未來也可能嘗試製作圖像或視覺生成相關的演示程式。</p><h2>中英翻譯演示程式的實現細節</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_150.jpg" /></p><p>作者整理了14個關於Gradio的<strong>基本問題</strong>，旨在幫助學習者快速了解Gradio的安裝、最簡單程式的樣貌、程式提供的三種訪問方式，以及Interface和各種組件的使用，以便進行<strong>自定義界面</strong>的製作。作為第二個共學任務，中英翻譯Demo的製作分為兩大步驟：首先是製作了<strong>非串流處理</strong>版本，其中大語言模型採用OpenAI的GPT-OSS 20B，可透過VLLM或Ollama部署，並透過OpenAI開發庫進行訪問；Gradio的界面部分程式碼極為簡潔。第二步則是製作了<strong>串流版本</strong>，主要透過將<code>stream</code>參數設為<code>true</code>並進行讀取來實現。</p><h2>OpenAI API與演示程式的存取</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_4.jpg" /></p><p>自今年三月起，OpenAI的API開發庫最新提供了<strong>response API</strong>，目前官方文檔也多採用此API實現，相較於舊的chat completions API，它支援更多功能，如檔案檢索和計算機使用等。作者對這兩款API進行了對比，並分別實現了它們的串流版本。關於這個中英翻譯的演示程式，作者已部署好並提供存取地址，但由於使用了租賃的雲伺服器，該服務將提供約一天時間供試用。所有相關文檔和程式碼將上傳至知識星球，並可於作者的Bilibili小店下載。</p><h2>未來內容預告</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_5.jpg" /></p><p>作者預告，下期影片將分享如何製作<strong>語音識別</strong>和<strong>語音合成</strong>的演示程式，特別是如何製作一個類似<strong>有聲小說</strong>的<strong>多角色語音合成</strong>Demo。</p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是这几天我新制作的。模仿谷歌翻译的一个小的演示程序。背后呢使用的是OpenAI最新发布的。GPT-OSS 20 b的大模型。这个演示程序。通过使用Gradio。它的核心代码其实都不超过10行。并且呢。可以一键就分享为这么一个Web服务。可以非常方便的让人来访问和使用。那么它是如何实现的呢。这个翻译的效果又如何呢。这个翻译的小demo呢。我会在本视频发布之后的一天之内呢。进行公开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">感兴趣的小伙伴可以亲自来试一试。即便是赶不上这次Demo呢。也没有关系。所有的教程和代码呢都会打包好。你也可以参考他们。来尝试制作并部署自己的Web服务。这个小的演示程序呢。是我们频道。最新的一个共学题目的一部分。也就是呢。使用这个Gradio。来制作基于大模型的演示程序。现在的大语言模型呢。基本都是基于Python来制作的。一个基于Python的程序。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要制作Demo进行分享还是有些复杂的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_59.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先它需要安装Python的运行环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外现在的大模型。对机器的要求呢是比较高的。比如这个GPT-OSS这款模型。就至少需要16G显存以上的GPU。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后一点呢就是Python呢。它默认是命令行的交互方式。这一点对一般的用户。是非常的不友好。而这个Gradio呢。就是为了解决这个问题的。通过它呢。你可以非常轻松的。构建这种交互式的AI演示程序。并且呢可以非常方。便的分享出去。这一次我们共学内容的安排呢。一个就是学习这个Gradio的基本知识。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。就是用它呢来做一些小的demo程序。目前呢。我计划制作基于文本处理的demo。也就是今天介绍的这个翻译的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外呢还会继续制作语音识别的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以及语音合成的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后面看看。有时间的话呢。也会尝试一下制作图像生成。视觉生成的相关的一些演示程序。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先关于这个Gradio的一些基本知识呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里我整理了14个。关于Gradio的基本问题。通过这十几个问题呢。你就可以对这个Gradio。有一个初步的了解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这里如何进行安装。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一个最简单的程序长什么样子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_121.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这边呢总结了。Gradio程序提供的三种访问的方式。再然后呢。就是它提供的INTERFACE以及组件。你也可以呢。使用这些组件呢。来进行自定义界面的制作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为这次共学的第二个任务呢。就是制作这个中英翻译的demo。我这边这个demo的制作呢。大体分为两步。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一步呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是制作了一个。非streaming处理的一个版本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢大语言模型呢。就是使用的这个OpenAI的。GPT-OSS的20B的模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要使用这个模型呢。你可以通过VLLM或者呢Ollama来部署它。然后通过OpenAI的这个开发库。就可以来访问它了。Gradio提供的界面部分呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实制作起来非常的简单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就这么几行简单的代码就可以。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二步呢就是我制作了一个streaming。版这个代码呢。看上去也不复杂。就是将这个stream这个参数呢改为true。然后呢。就可以这样呢进行读取来实现了。从今年3月份开始呢。OpenAI的API开发库呢。它最新提供了response的API。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">目前OpenAI的官方文档呢。基本。都在使用这个最新的API来实现的。相比这个chat completions API呢。这个responses API它支持的功能就更多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_191.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如这个文件检索。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">计算机的使用。MCP等等一些最新的功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么这两个版本有什么样的差别呢。这里呢。我对这两个API呢进行了一个对比。然后呢。是分别实现了它们的streaming版本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于这个中英翻译的演示程序呢。我这边已经部署好了。大家可以在视频下面的置顶留言。或者交流群中呢。去获取这个访问的地址。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个服务呢。使用的是一个租赁的云服务器。所以呢不会一直提供。大语言呢会提供一天左右的时间。供小伙伴们呢访问和尝试。所有的文档和代码呢。也会上传到我们的知识星球当中。没有加入星球的小伙伴呢。也也可以去我的b站小店里去下载。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">下期视频。我会分享一下如何制作语音识别。语音合成的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">尤其是。我们来试试看看。如何来制作一个类似有声小说的。这种多角色语音合成的demo。好的以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>用Gradio+GPT-OSS做了个翻译器，效果吊打谷歌？</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/Odvl5jeFNJk/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=Odvl5jeFNJk">https://www.youtube.com/watch?v=Odvl5jeFNJk</a></p><h1>值得閱讀的理由</h1><ul><li>了解如何使用Gradio輕鬆構建基於大型模型的互動式AI演示程式。</li><li>深入理解OpenAI GPT-OSS 20B模型在文本翻譯應用中的實踐，特別是串流處理的實現方式。</li><li>掌握作者關於語音識別、語音合成及多角色語音生成等未來AI應用開發的詳細計畫。</li></ul><hr /><h1>摘要</h1><h2>引言：Gradio與GPT-OSS 20B的創新結合</h2><p>作者近日展示了一個模仿Google翻譯的小型演示程式，其背後採用了OpenAI最新發布的GPT-OSS 20B大模型。這個程式透過Gradio開發，核心程式碼不超過十行，且能一鍵分享為方便大眾訪問和使用的Web服務。作者預告，該翻譯demo將在本影片發布後一天內公開，並會提供完整的教學和程式碼。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_25.jpg" /></p><p>這個演示程式是作者頻道「共學題目」的一部分，旨在探討如何使用Gradio製作基於大模型的演示程式。作者指出，雖然目前大語言模型多基於Python開發，但Python程式要製作可分享的Demo面臨多重挑戰。</p><h2>AI模型演示的挑戰與Gradio的解決方案</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_55.jpg" /></p><p>這些挑戰包括：首先，使用者需要安裝Python運行環境；其次，當前大模型對硬體要求高，例如GPT-OSS模型至少需要16G顯存的GPU；最後，Python預設的命令列交互方式對一般用戶來說極不友善。為了解決這些問題，作者引入了<strong>Gradio</strong>，它能讓開發者輕鬆構建<strong>交互式AI演示程式</strong>，並方便地分享出去。</p><h2>Gradio學習與未來演示計畫</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_115.jpg" /></p><p>本次共學內容規劃包含學習Gradio的<strong>基本知識</strong>，並運用其製作多種小型Demo。目前，作者計畫製作基於文本處理的翻譯Demo（即影片中介紹的翻譯器），此外還將陸續開發<strong>語音識別</strong>和<strong>語音合成</strong>Demo，未來也可能嘗試製作圖像或視覺生成相關的演示程式。</p><h2>中英翻譯演示程式的實現細節</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_150.jpg" /></p><p>作者整理了14個關於Gradio的<strong>基本問題</strong>，旨在幫助學習者快速了解Gradio的安裝、最簡單程式的樣貌、程式提供的三種訪問方式，以及Interface和各種組件的使用，以便進行<strong>自定義界面</strong>的製作。作為第二個共學任務，中英翻譯Demo的製作分為兩大步驟：首先是製作了<strong>非串流處理</strong>版本，其中大語言模型採用OpenAI的GPT-OSS 20B，可透過VLLM或Ollama部署，並透過OpenAI開發庫進行訪問；Gradio的界面部分程式碼極為簡潔。第二步則是製作了<strong>串流版本</strong>，主要透過將<code>stream</code>參數設為<code>true</code>並進行讀取來實現。</p><h2>OpenAI API與演示程式的存取</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_4.jpg" /></p><p>自今年三月起，OpenAI的API開發庫最新提供了<strong>response API</strong>，目前官方文檔也多採用此API實現，相較於舊的chat completions API，它支援更多功能，如檔案檢索和計算機使用等。作者對這兩款API進行了對比，並分別實現了它們的串流版本。關於這個中英翻譯的演示程式，作者已部署好並提供存取地址，但由於使用了租賃的雲伺服器，該服務將提供約一天時間供試用。所有相關文檔和程式碼將上傳至知識星球，並可於作者的Bilibili小店下載。</p><h2>未來內容預告</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_5.jpg" /></p><p>作者預告，下期影片將分享如何製作<strong>語音識別</strong>和<strong>語音合成</strong>的演示程式，特別是如何製作一個類似<strong>有聲小說</strong>的<strong>多角色語音合成</strong>Demo。</p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是这几天我新制作的。模仿谷歌翻译的一个小的演示程序。背后呢使用的是OpenAI最新发布的。GPT-OSS 20 b的大模型。这个演示程序。通过使用Gradio。它的核心代码其实都不超过10行。并且呢。可以一键就分享为这么一个Web服务。可以非常方便的让人来访问和使用。那么它是如何实现的呢。这个翻译的效果又如何呢。这个翻译的小demo呢。我会在本视频发布之后的一天之内呢。进行公开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">感兴趣的小伙伴可以亲自来试一试。即便是赶不上这次Demo呢。也没有关系。所有的教程和代码呢都会打包好。你也可以参考他们。来尝试制作并部署自己的Web服务。这个小的演示程序呢。是我们频道。最新的一个共学题目的一部分。也就是呢。使用这个Gradio。来制作基于大模型的演示程序。现在的大语言模型呢。基本都是基于Python来制作的。一个基于Python的程序。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要制作Demo进行分享还是有些复杂的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_59.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先它需要安装Python的运行环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外现在的大模型。对机器的要求呢是比较高的。比如这个GPT-OSS这款模型。就至少需要16G显存以上的GPU。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后一点呢就是Python呢。它默认是命令行的交互方式。这一点对一般的用户。是非常的不友好。而这个Gradio呢。就是为了解决这个问题的。通过它呢。你可以非常轻松的。构建这种交互式的AI演示程序。并且呢可以非常方。便的分享出去。这一次我们共学内容的安排呢。一个就是学习这个Gradio的基本知识。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。就是用它呢来做一些小的demo程序。目前呢。我计划制作基于文本处理的demo。也就是今天介绍的这个翻译的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外呢还会继续制作语音识别的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以及语音合成的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后面看看。有时间的话呢。也会尝试一下制作图像生成。视觉生成的相关的一些演示程序。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先关于这个Gradio的一些基本知识呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里我整理了14个。关于Gradio的基本问题。通过这十几个问题呢。你就可以对这个Gradio。有一个初步的了解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这里如何进行安装。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一个最简单的程序长什么样子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_121.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这边呢总结了。Gradio程序提供的三种访问的方式。再然后呢。就是它提供的INTERFACE以及组件。你也可以呢。使用这些组件呢。来进行自定义界面的制作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为这次共学的第二个任务呢。就是制作这个中英翻译的demo。我这边这个demo的制作呢。大体分为两步。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一步呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是制作了一个。非streaming处理的一个版本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢大语言模型呢。就是使用的这个OpenAI的。GPT-OSS的20B的模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要使用这个模型呢。你可以通过VLLM或者呢Ollama来部署它。然后通过OpenAI的这个开发库。就可以来访问它了。Gradio提供的界面部分呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实制作起来非常的简单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就这么几行简单的代码就可以。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二步呢就是我制作了一个streaming。版这个代码呢。看上去也不复杂。就是将这个stream这个参数呢改为true。然后呢。就可以这样呢进行读取来实现了。从今年3月份开始呢。OpenAI的API开发库呢。它最新提供了response的API。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">目前OpenAI的官方文档呢。基本。都在使用这个最新的API来实现的。相比这个chat completions API呢。这个responses API它支持的功能就更多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_191.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如这个文件检索。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">计算机的使用。MCP等等一些最新的功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么这两个版本有什么样的差别呢。这里呢。我对这两个API呢进行了一个对比。然后呢。是分别实现了它们的streaming版本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于这个中英翻译的演示程序呢。我这边已经部署好了。大家可以在视频下面的置顶留言。或者交流群中呢。去获取这个访问的地址。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个服务呢。使用的是一个租赁的云服务器。所以呢不会一直提供。大语言呢会提供一天左右的时间。供小伙伴们呢访问和尝试。所有的文档和代码呢。也会上传到我们的知识星球当中。没有加入星球的小伙伴呢。也也可以去我的b站小店里去下载。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">下期视频。我会分享一下如何制作语音识别。语音合成的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">尤其是。我们来试试看看。如何来制作一个类似有声小说的。这种多角色语音合成的demo。好的以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>用Gradio+GPT-OSS做了个翻译器，效果吊打谷歌？</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/Odvl5jeFNJk/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=Odvl5jeFNJk">https://www.youtube.com/watch?v=Odvl5jeFNJk</a></p><h1>值得閱讀的理由</h1><ul><li>了解如何使用Gradio輕鬆構建基於大型模型的互動式AI演示程式。</li><li>深入理解OpenAI GPT-OSS 20B模型在文本翻譯應用中的實踐，特別是串流處理的實現方式。</li><li>掌握作者關於語音識別、語音合成及多角色語音生成等未來AI應用開發的詳細計畫。</li></ul><hr /><h1>摘要</h1><h2>引言：Gradio與GPT-OSS 20B的創新結合</h2><p>作者近日展示了一個模仿Google翻譯的小型演示程式，其背後採用了OpenAI最新發布的GPT-OSS 20B大模型。這個程式透過Gradio開發，核心程式碼不超過十行，且能一鍵分享為方便大眾訪問和使用的Web服務。作者預告，該翻譯demo將在本影片發布後一天內公開，並會提供完整的教學和程式碼。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_25.jpg" /></p><p>這個演示程式是作者頻道「共學題目」的一部分，旨在探討如何使用Gradio製作基於大模型的演示程式。作者指出，雖然目前大語言模型多基於Python開發，但Python程式要製作可分享的Demo面臨多重挑戰。</p><h2>AI模型演示的挑戰與Gradio的解決方案</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_55.jpg" /></p><p>這些挑戰包括：首先，使用者需要安裝Python運行環境；其次，當前大模型對硬體要求高，例如GPT-OSS模型至少需要16G顯存的GPU；最後，Python預設的命令列交互方式對一般用戶來說極不友善。為了解決這些問題，作者引入了<strong>Gradio</strong>，它能讓開發者輕鬆構建<strong>交互式AI演示程式</strong>，並方便地分享出去。</p><h2>Gradio學習與未來演示計畫</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_115.jpg" /></p><p>本次共學內容規劃包含學習Gradio的<strong>基本知識</strong>，並運用其製作多種小型Demo。目前，作者計畫製作基於文本處理的翻譯Demo（即影片中介紹的翻譯器），此外還將陸續開發<strong>語音識別</strong>和<strong>語音合成</strong>Demo，未來也可能嘗試製作圖像或視覺生成相關的演示程式。</p><h2>中英翻譯演示程式的實現細節</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_150.jpg" /></p><p>作者整理了14個關於Gradio的<strong>基本問題</strong>，旨在幫助學習者快速了解Gradio的安裝、最簡單程式的樣貌、程式提供的三種訪問方式，以及Interface和各種組件的使用，以便進行<strong>自定義界面</strong>的製作。作為第二個共學任務，中英翻譯Demo的製作分為兩大步驟：首先是製作了<strong>非串流處理</strong>版本，其中大語言模型採用OpenAI的GPT-OSS 20B，可透過VLLM或Ollama部署，並透過OpenAI開發庫進行訪問；Gradio的界面部分程式碼極為簡潔。第二步則是製作了<strong>串流版本</strong>，主要透過將<code>stream</code>參數設為<code>true</code>並進行讀取來實現。</p><h2>OpenAI API與演示程式的存取</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_4.jpg" /></p><p>自今年三月起，OpenAI的API開發庫最新提供了<strong>response API</strong>，目前官方文檔也多採用此API實現，相較於舊的chat completions API，它支援更多功能，如檔案檢索和計算機使用等。作者對這兩款API進行了對比，並分別實現了它們的串流版本。關於這個中英翻譯的演示程式，作者已部署好並提供存取地址，但由於使用了租賃的雲伺服器，該服務將提供約一天時間供試用。所有相關文檔和程式碼將上傳至知識星球，並可於作者的Bilibili小店下載。</p><h2>未來內容預告</h2><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_5.jpg" /></p><p>作者預告，下期影片將分享如何製作<strong>語音識別</strong>和<strong>語音合成</strong>的演示程式，特別是如何製作一個類似<strong>有聲小說</strong>的<strong>多角色語音合成</strong>Demo。</p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是这几天我新制作的。模仿谷歌翻译的一个小的演示程序。背后呢使用的是OpenAI最新发布的。GPT-OSS 20 b的大模型。这个演示程序。通过使用Gradio。它的核心代码其实都不超过10行。并且呢。可以一键就分享为这么一个Web服务。可以非常方便的让人来访问和使用。那么它是如何实现的呢。这个翻译的效果又如何呢。这个翻译的小demo呢。我会在本视频发布之后的一天之内呢。进行公开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">感兴趣的小伙伴可以亲自来试一试。即便是赶不上这次Demo呢。也没有关系。所有的教程和代码呢都会打包好。你也可以参考他们。来尝试制作并部署自己的Web服务。这个小的演示程序呢。是我们频道。最新的一个共学题目的一部分。也就是呢。使用这个Gradio。来制作基于大模型的演示程序。现在的大语言模型呢。基本都是基于Python来制作的。一个基于Python的程序。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要制作Demo进行分享还是有些复杂的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_59.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先它需要安装Python的运行环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外现在的大模型。对机器的要求呢是比较高的。比如这个GPT-OSS这款模型。就至少需要16G显存以上的GPU。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后一点呢就是Python呢。它默认是命令行的交互方式。这一点对一般的用户。是非常的不友好。而这个Gradio呢。就是为了解决这个问题的。通过它呢。你可以非常轻松的。构建这种交互式的AI演示程序。并且呢可以非常方。便的分享出去。这一次我们共学内容的安排呢。一个就是学习这个Gradio的基本知识。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。就是用它呢来做一些小的demo程序。目前呢。我计划制作基于文本处理的demo。也就是今天介绍的这个翻译的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外呢还会继续制作语音识别的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以及语音合成的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后面看看。有时间的话呢。也会尝试一下制作图像生成。视觉生成的相关的一些演示程序。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先关于这个Gradio的一些基本知识呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里我整理了14个。关于Gradio的基本问题。通过这十几个问题呢。你就可以对这个Gradio。有一个初步的了解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这里如何进行安装。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一个最简单的程序长什么样子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_121.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这边呢总结了。Gradio程序提供的三种访问的方式。再然后呢。就是它提供的INTERFACE以及组件。你也可以呢。使用这些组件呢。来进行自定义界面的制作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为这次共学的第二个任务呢。就是制作这个中英翻译的demo。我这边这个demo的制作呢。大体分为两步。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一步呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是制作了一个。非streaming处理的一个版本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢大语言模型呢。就是使用的这个OpenAI的。GPT-OSS的20B的模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要使用这个模型呢。你可以通过VLLM或者呢Ollama来部署它。然后通过OpenAI的这个开发库。就可以来访问它了。Gradio提供的界面部分呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实制作起来非常的简单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就这么几行简单的代码就可以。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二步呢就是我制作了一个streaming。版这个代码呢。看上去也不复杂。就是将这个stream这个参数呢改为true。然后呢。就可以这样呢进行读取来实现了。从今年3月份开始呢。OpenAI的API开发库呢。它最新提供了response的API。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">目前OpenAI的官方文档呢。基本。都在使用这个最新的API来实现的。相比这个chat completions API呢。这个responses API它支持的功能就更多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/Odvl5jeFNJk_191.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如这个文件检索。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">计算机的使用。MCP等等一些最新的功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么这两个版本有什么样的差别呢。这里呢。我对这两个API呢进行了一个对比。然后呢。是分别实现了它们的streaming版本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于这个中英翻译的演示程序呢。我这边已经部署好了。大家可以在视频下面的置顶留言。或者交流群中呢。去获取这个访问的地址。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个服务呢。使用的是一个租赁的云服务器。所以呢不会一直提供。大语言呢会提供一天左右的时间。供小伙伴们呢访问和尝试。所有的文档和代码呢。也会上传到我们的知识星球当中。没有加入星球的小伙伴呢。也也可以去我的b站小店里去下载。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">下期视频。我会分享一下如何制作语音识别。语音合成的demo。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">尤其是。我们来试试看看。如何来制作一个类似有声小说的。这种多角色语音合成的demo。好的以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>用Gradio+GPT-OSS做了个翻译器，效果吊打谷歌？</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/Odvl5jeFNJk/hqdefault.jpg"/>
      <pubDate>2025-08-23T13:22:02.000Z</pubDate>
    </item><item>
      <title><![CDATA[GPT-OSS 本地部署实测，120B与20B如何做到同样快的？]]></title>
      <link>https://www.youtube.com/watch?v=vz8np0yiNvw</link>
      <itunes:title><![CDATA[GPT-OSS 本地部署实测，120B与20B如何做到同样快的？]]></itunes:title>
      <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/vz8np0yiNvw/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=vz8np0yiNvw">https://www.youtube.com/watch?v=vz8np0yiNvw</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解 OpenAI 開源的 GPT-OSS 混合專家模型 (MoE) 的技術特性與設計理念。</li> <li>學習如何在本地端部署這些大型語言模型，並了解不同部署方式的適用情境。</li> <li>比較 20B 和 120B 兩種模型在能力、顯存佔用和推理速度上的實際表現，特別是其高效能的背後原因。</li> </ul> <hr /> <h1>摘要</h1> <h2>OpenAI GPT-OSS 模型概述與技術特點</h2> <p>影片開頭，作者介紹了 OpenAI 開源的 <strong>GPT-OSS</strong> 模型，並分享其在本地部署與測試這兩款模型的心得。首先，作者透過技術報告深入探討了這些模型的關鍵特性。這些開源模型具備出色的指令遵循、工具使用、基於 CoT (Chain of Thought) 的深度推理以及結構化輸出能力。然而，儘管模型能力全面且在排行榜上表現不錯，但 <strong>OpenAI 刻意將其能力限制在非「高能力」水平</strong>，以防止惡意濫用。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_0.jpg" /></p></p> <p>在模型參數分佈方面，<strong>MLP (多層感知器) 部分佔據了高達 90% 的參數</strong>，這正是 MoE 模型中「專家部分」的核心。為此，OpenAI 進一步對 MoE 部分進行了後訓練階段的 <strong>MXFP4 格式量化</strong>，將每個參數量化到 4.25 比特。這使得 120B 模型能在 80G 顯卡上運行，而 20B 模型僅需 16G 顯卡。作者也提到，120B 和 20B 模型分別擁有 128 和 32 個專家，但每個 TOKEN 處理時只會同時激活四個專家，這意味著儘管總參數量差距大，但實際激活的參數量差異不大。此外，該模型採用了與 Hugging Face tokenizer 不同的獨特 <strong>tokenizer</strong>。訓練時間方面，120B 模型需 210 萬小時 H100 GPU 時間，20B 模型約 20 萬小時，相當於使用 32 台各帶 8 張 H100 GPU 的機器訓練約一個月。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_30.jpg" /></p></p> <hr /> <h2>本地部署方式與實踐</h2> <p>影片接著介紹了多種本地部署 <strong>GPT-OSS</strong> 模型的方式。最簡便的是使用 <strong>Ollama</strong>，此外也可參考 <strong>unsloth</strong> 或 <strong>OpenAI 官方推薦的方式</strong>。作者主要採用 OpenAI 官方方式，其提供了兩種部署模式：一種是基於 <strong>vLLM Serve</strong> 的方式，適用於將模型部署為對話模型或進行 Demo 演示；另一種是<strong>非 Serve 模式</strong>，更適合進行大量的並行數據處理。這兩種方式都基於 <strong>vLLM</strong> 框架，因此安裝步驟大致相同。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_240.jpg" /></p></p> <hr /> <h2>模型性能測試與對比</h2> <p>作者準備了三個實例來測試模型。首先是讓模型撰寫一份關於特朗普的介紹：20B 模型的內容問題較多且中文表達奇怪，而 120B 模型則表現出色，明顯展現了兩者在能力上的巨大差距。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_360.jpg" /></p></p> <p>其次是測試模型呼叫外部工具的功能，例如查詢北京天氣。20B 模型起初無法正確解答，後經調整中文編碼才解決，這顯示其對中文編碼的敏感性；而 120B 模型則對中文編碼不敏感，一直能正確回答，再次印證了參數量對模型能力（特別是中文處理）的顯著影響。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_420.jpg" /></p></p> <p>第三個實例是基於 OpenAI agent 的實現，該實現將複雜的程式碼進行了封裝，最終程式碼更為簡潔。這部分測試中，兩個模型都能順利運行。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_8.jpg" /></p></p> <p>最後，作者提供了一份詳細的 20B 與 120B 模型對比：</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>1.  <strong>模型能力：</strong> 毫無疑問，120B 模型遠勝於 20B 模型，尤其在處理 <strong>中文</strong> 的可用性上，120B 表現更佳。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>2.  <strong>顯存佔用：</strong> 實測顯示，20B 模型佔用約 14GB 顯存，120B 模型約佔用 65GB 顯存。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>3.  <strong>推理速度：</strong> 這是最令人驚訝的結果，20B 模型每秒約處理 155 個 TOKEN，而 120B 模型每秒約 141 個 TOKEN。儘管總參數量相差六倍，但 <strong>推理速度</strong> 的差距卻非常小。作者解釋，這是因為兩個模型在推理時實際 <strong>激活的參數量</strong> 差距不大（120B 約 5.13B，20B 約 3.61B）。因此，儘管 120B 模型顯存佔用較高，但其快速的推理速度使其非常適合在本地進行部署和運行。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_10.jpg" /></p></p> <p>影片結尾，作者提到相關的本地部署說明和程式碼可在特定平台下載。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_10.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好上周呢。OpenAI开源了GPT-OSS模型。这周呢我在本地部署。并测试了一下这两款模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就来为大家介绍一下这两款模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">来看一下如何在本地部署。以及。他们在本地部署时的推理速度的差距。到底有多大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先我们先通过他的技术报告呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">来看一下这个模型的一些特点。关于这个模型。下面几点呢。是需要我们了解的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个就是OpenAI的这款开源模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">支持的能力还是非常全的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">包括出色的指令遵循能力。工具使用能力。基于CoT的深度推理能力。以及结构化的输出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能力比较全面。刷榜性能呢。也不错但是呢。这个模型的能力。算不上强。根据OpenAI技术报告中的说明。即便是对这个120B的模型进行微调。这个模型的能力。也无法达到高能力。那他这里的高能力。并不是说这个模型刷榜的能力不行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是呢针对我们真实使用时。一些复杂场景中的一个处理的能力。即使你看他们这个刷榜的结果。他的这个数值。还是非常不错的。那么为什么模型达不到高能力呢。OpenAI的解释。是他们故意为之的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_66.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">原因呢就是防止。这个模型能力太强。会被恶意的使用。第三点就是模型参数的分布。不同参数量的模型。它到底是哪些部分。占用了这么大的参数量呢。这个呢我们可以通过这个表。就可以知道。也就是说MLP这部分。它占用了是90%的参数。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实这一部分呢就。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是我们常说的。这个MoE模型的专家部分。所对应的一些层。所以呢第四点就来了。OpenAI呢就进一步。对这个模型的MoE的部分。进行了量化处理。通过在后训练阶段呢。将MoE的参数。进行这个MXFP4格式的量化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是将每个参数量化到4.25比特。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那量化后呢。120B的模型可以在80G的显卡上运行。而20B的模型呢。只需要16G的显卡就可以运行了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第五点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是大家可以看一下。这个模型的专家数量。这个120B和20B的两款模型呢。分别的专家数量呢是128和32。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个TOKEN处理时呢。同时只会激活四个专家。所以说呢。你可以看到这个地方。这两款模型。虽然说它的参数量差距很大。但是它的激活的参数量呢。差别是不是很大的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_133.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第六点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个模型的tokenizer是比较特殊的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个它与我们常用的。这个基于huggingface tokenizer。是有些不同的。这个是它一个比较独特的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第七点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个模型训练所使用的时间。它这边的时间呢。是基于H100 GPU进行计算的。120B的模型呢是需要210万个小时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而20B的模型呢。大约是需要20万小时左右。那大约是一个什么样的计算量呢。假设你使用32台。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每台机器呢。带8张H100 GPU的机器进行训练的话呢。大约呢就需要一个月的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那关于第八点呢。就是这模型。关于模型安全。幻觉等评价相关的内容。还是做了很多介绍的对。这方面内容感兴趣的小伙伴呢。可以详细的阅读一下。这个技术报告的后面部分。好以上呢。就是这个技术报告当中呢。介绍的一些主要的内容。下面呢给大家介绍一下。如何在本地部署运行这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要使用这个模型。目前有多种方式。第一个呢。最简单的方式呢。就是你可以直接使用Ollama呢。来下载并运行它。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_196.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是你也可以参考这个unsloth提供的。这个本地部署的说明。来运行它。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个呢。就是参考OpenAI官方推荐的方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我这边呢。主要参考的是OpenAI官方的方式。来运行的这个模型。那这边的介绍呢。它提供的是两种方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是基于这个vLLM Serve的一种方式。另一种呢是一种非Serve的方式。那这两种方式有什么样的差别呢。第一个就是。如果你希望将这个模型部署后。作为对话模型使用。或者是呢。用于做一些Demo的演示。那么你可以使用这个基于Serve的方式。那如果呢。你希望做一些大量的数据处理。尤其是。进行并行的处理的话呢。这个时候。使用这个飞serve的方式呢。就会更方便一些。那这两种部署方式呢。它都是基于这个vLLM的。所以安装起来呢。基本上是一样的步骤。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">参考他这个代码呢。我准备了三个例子。第一个呢。是我让他写一个关于特朗普的介绍。这边呢是这个20B模型写出来的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里一看。就有很多的问题。很多的中文表达上。也是有些。奇怪但是你再看一下。这个120B模型写的这篇介绍。看起来呢。就非常的不错了。那第二个呢。就是我测试了一下。他调用外部工具的一个功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_267.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这边给出的这个例子。它有一些问题。而这边呢。是参考了其他的代码。来重新写了一个完整的大语言模型。调用外部工具的代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个代码呢。其实也很简单。问题呢就是问一下北京的天气。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">数据获取呢。是通过这个函数。直接将这个结果返回了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在实际的应用时。你可以通过API。或者抓取网页数据的方式呢。来获取真实的实时的数据。我分别测了一下这个20B和120B的模型。这个20B的模型。最开始呢。一直是没法正确的解答这个问题。后来我发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实是这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他对中文的编码呢比较敏感。后来调整一下这个中文的编码呢。就OK了但是这个120B的模型。对中文的编码就不敏感。一直呢是可以进行正确回答的。从这一点呢。就可以看出来。这个模型的参数量。对模型能力的影响。还真是挺大的。那第三个代码呢。就是基于这个OpenAI agent的一个实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">agent的实现呢。你可以简单的理解为。就是将这一坨的代码都给封装了起来。所以最终的代码看起来就简洁多了。那这个代码在两个模型上运行呢。都是没有任何问题的。最后呢。是我整理的一个非Serve的运行方式。那这个脚本呢。开始执行时呢。会有一个模型导入的一个过程。所以最开始。的执行。时间还是有些长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_339.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后我列了一下。这两个模型的一个简单的对比。关于这两个模型的能力呢。这个毋庸置疑。120B的模型。很明显是强过这个20B的模型。尤其是在针对这个中文的处理能力上。120B的模型的可用性呢。还是比较高的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是我关注的这个显存占用。在我这个实测当中呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个20B的模型。占用的显存呢是14个G。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而120B的模型呢。占用显存。大约是65个G。最后点呢。就是推理速度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">推理速度呢。20B的模型每秒呢。大约是155个TOKEN。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而120B的模型呢。每秒是141个token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我觉得这个结果。还是非常有意思的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是你可以看到。这个模型的参数量的对比。是120B和20B参数量相差呢是6倍的关系。但是它的推理速度。可以说差距是非常的小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那之所以这样呢。就是因为。它每次推理时。它的激活的参数量差距是非常小的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可以看到120B的模型呢。它是5.13B。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_401.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而20B呢是3.61B。这个120B的模型。它的这个显存占用呢。只需要65G左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">推理速度呢。还是非常的快的。所以还是非常适合。在本地进行部署运行的。本地部署相关的说明和代码呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家可以去星球。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">或者我们的b站小店中呢。去下载好。以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>GPT-OSS 本地部署实测，120B与20B如何做到同样快的？</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/vz8np0yiNvw/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=vz8np0yiNvw">https://www.youtube.com/watch?v=vz8np0yiNvw</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解 OpenAI 開源的 GPT-OSS 混合專家模型 (MoE) 的技術特性與設計理念。</li> <li>學習如何在本地端部署這些大型語言模型，並了解不同部署方式的適用情境。</li> <li>比較 20B 和 120B 兩種模型在能力、顯存佔用和推理速度上的實際表現，特別是其高效能的背後原因。</li> </ul> <hr /> <h1>摘要</h1> <h2>OpenAI GPT-OSS 模型概述與技術特點</h2> <p>影片開頭，作者介紹了 OpenAI 開源的 <strong>GPT-OSS</strong> 模型，並分享其在本地部署與測試這兩款模型的心得。首先，作者透過技術報告深入探討了這些模型的關鍵特性。這些開源模型具備出色的指令遵循、工具使用、基於 CoT (Chain of Thought) 的深度推理以及結構化輸出能力。然而，儘管模型能力全面且在排行榜上表現不錯，但 <strong>OpenAI 刻意將其能力限制在非「高能力」水平</strong>，以防止惡意濫用。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_0.jpg" /></p></p> <p>在模型參數分佈方面，<strong>MLP (多層感知器) 部分佔據了高達 90% 的參數</strong>，這正是 MoE 模型中「專家部分」的核心。為此，OpenAI 進一步對 MoE 部分進行了後訓練階段的 <strong>MXFP4 格式量化</strong>，將每個參數量化到 4.25 比特。這使得 120B 模型能在 80G 顯卡上運行，而 20B 模型僅需 16G 顯卡。作者也提到，120B 和 20B 模型分別擁有 128 和 32 個專家，但每個 TOKEN 處理時只會同時激活四個專家，這意味著儘管總參數量差距大，但實際激活的參數量差異不大。此外，該模型採用了與 Hugging Face tokenizer 不同的獨特 <strong>tokenizer</strong>。訓練時間方面，120B 模型需 210 萬小時 H100 GPU 時間，20B 模型約 20 萬小時，相當於使用 32 台各帶 8 張 H100 GPU 的機器訓練約一個月。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_30.jpg" /></p></p> <hr /> <h2>本地部署方式與實踐</h2> <p>影片接著介紹了多種本地部署 <strong>GPT-OSS</strong> 模型的方式。最簡便的是使用 <strong>Ollama</strong>，此外也可參考 <strong>unsloth</strong> 或 <strong>OpenAI 官方推薦的方式</strong>。作者主要採用 OpenAI 官方方式，其提供了兩種部署模式：一種是基於 <strong>vLLM Serve</strong> 的方式，適用於將模型部署為對話模型或進行 Demo 演示；另一種是<strong>非 Serve 模式</strong>，更適合進行大量的並行數據處理。這兩種方式都基於 <strong>vLLM</strong> 框架，因此安裝步驟大致相同。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_240.jpg" /></p></p> <hr /> <h2>模型性能測試與對比</h2> <p>作者準備了三個實例來測試模型。首先是讓模型撰寫一份關於特朗普的介紹：20B 模型的內容問題較多且中文表達奇怪，而 120B 模型則表現出色，明顯展現了兩者在能力上的巨大差距。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_360.jpg" /></p></p> <p>其次是測試模型呼叫外部工具的功能，例如查詢北京天氣。20B 模型起初無法正確解答，後經調整中文編碼才解決，這顯示其對中文編碼的敏感性；而 120B 模型則對中文編碼不敏感，一直能正確回答，再次印證了參數量對模型能力（特別是中文處理）的顯著影響。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_420.jpg" /></p></p> <p>第三個實例是基於 OpenAI agent 的實現，該實現將複雜的程式碼進行了封裝，最終程式碼更為簡潔。這部分測試中，兩個模型都能順利運行。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_8.jpg" /></p></p> <p>最後，作者提供了一份詳細的 20B 與 120B 模型對比：</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>1.  <strong>模型能力：</strong> 毫無疑問，120B 模型遠勝於 20B 模型，尤其在處理 <strong>中文</strong> 的可用性上，120B 表現更佳。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>2.  <strong>顯存佔用：</strong> 實測顯示，20B 模型佔用約 14GB 顯存，120B 模型約佔用 65GB 顯存。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>3.  <strong>推理速度：</strong> 這是最令人驚訝的結果，20B 模型每秒約處理 155 個 TOKEN，而 120B 模型每秒約 141 個 TOKEN。儘管總參數量相差六倍，但 <strong>推理速度</strong> 的差距卻非常小。作者解釋，這是因為兩個模型在推理時實際 <strong>激活的參數量</strong> 差距不大（120B 約 5.13B，20B 約 3.61B）。因此，儘管 120B 模型顯存佔用較高，但其快速的推理速度使其非常適合在本地進行部署和運行。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_10.jpg" /></p></p> <p>影片結尾，作者提到相關的本地部署說明和程式碼可在特定平台下載。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_10.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好上周呢。OpenAI开源了GPT-OSS模型。这周呢我在本地部署。并测试了一下这两款模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就来为大家介绍一下这两款模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">来看一下如何在本地部署。以及。他们在本地部署时的推理速度的差距。到底有多大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先我们先通过他的技术报告呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">来看一下这个模型的一些特点。关于这个模型。下面几点呢。是需要我们了解的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个就是OpenAI的这款开源模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">支持的能力还是非常全的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">包括出色的指令遵循能力。工具使用能力。基于CoT的深度推理能力。以及结构化的输出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能力比较全面。刷榜性能呢。也不错但是呢。这个模型的能力。算不上强。根据OpenAI技术报告中的说明。即便是对这个120B的模型进行微调。这个模型的能力。也无法达到高能力。那他这里的高能力。并不是说这个模型刷榜的能力不行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是呢针对我们真实使用时。一些复杂场景中的一个处理的能力。即使你看他们这个刷榜的结果。他的这个数值。还是非常不错的。那么为什么模型达不到高能力呢。OpenAI的解释。是他们故意为之的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_66.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">原因呢就是防止。这个模型能力太强。会被恶意的使用。第三点就是模型参数的分布。不同参数量的模型。它到底是哪些部分。占用了这么大的参数量呢。这个呢我们可以通过这个表。就可以知道。也就是说MLP这部分。它占用了是90%的参数。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实这一部分呢就。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是我们常说的。这个MoE模型的专家部分。所对应的一些层。所以呢第四点就来了。OpenAI呢就进一步。对这个模型的MoE的部分。进行了量化处理。通过在后训练阶段呢。将MoE的参数。进行这个MXFP4格式的量化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是将每个参数量化到4.25比特。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那量化后呢。120B的模型可以在80G的显卡上运行。而20B的模型呢。只需要16G的显卡就可以运行了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第五点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是大家可以看一下。这个模型的专家数量。这个120B和20B的两款模型呢。分别的专家数量呢是128和32。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个TOKEN处理时呢。同时只会激活四个专家。所以说呢。你可以看到这个地方。这两款模型。虽然说它的参数量差距很大。但是它的激活的参数量呢。差别是不是很大的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_133.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第六点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个模型的tokenizer是比较特殊的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个它与我们常用的。这个基于huggingface tokenizer。是有些不同的。这个是它一个比较独特的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第七点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个模型训练所使用的时间。它这边的时间呢。是基于H100 GPU进行计算的。120B的模型呢是需要210万个小时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而20B的模型呢。大约是需要20万小时左右。那大约是一个什么样的计算量呢。假设你使用32台。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每台机器呢。带8张H100 GPU的机器进行训练的话呢。大约呢就需要一个月的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那关于第八点呢。就是这模型。关于模型安全。幻觉等评价相关的内容。还是做了很多介绍的对。这方面内容感兴趣的小伙伴呢。可以详细的阅读一下。这个技术报告的后面部分。好以上呢。就是这个技术报告当中呢。介绍的一些主要的内容。下面呢给大家介绍一下。如何在本地部署运行这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要使用这个模型。目前有多种方式。第一个呢。最简单的方式呢。就是你可以直接使用Ollama呢。来下载并运行它。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_196.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是你也可以参考这个unsloth提供的。这个本地部署的说明。来运行它。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个呢。就是参考OpenAI官方推荐的方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我这边呢。主要参考的是OpenAI官方的方式。来运行的这个模型。那这边的介绍呢。它提供的是两种方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是基于这个vLLM Serve的一种方式。另一种呢是一种非Serve的方式。那这两种方式有什么样的差别呢。第一个就是。如果你希望将这个模型部署后。作为对话模型使用。或者是呢。用于做一些Demo的演示。那么你可以使用这个基于Serve的方式。那如果呢。你希望做一些大量的数据处理。尤其是。进行并行的处理的话呢。这个时候。使用这个飞serve的方式呢。就会更方便一些。那这两种部署方式呢。它都是基于这个vLLM的。所以安装起来呢。基本上是一样的步骤。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">参考他这个代码呢。我准备了三个例子。第一个呢。是我让他写一个关于特朗普的介绍。这边呢是这个20B模型写出来的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里一看。就有很多的问题。很多的中文表达上。也是有些。奇怪但是你再看一下。这个120B模型写的这篇介绍。看起来呢。就非常的不错了。那第二个呢。就是我测试了一下。他调用外部工具的一个功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_267.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这边给出的这个例子。它有一些问题。而这边呢。是参考了其他的代码。来重新写了一个完整的大语言模型。调用外部工具的代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个代码呢。其实也很简单。问题呢就是问一下北京的天气。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">数据获取呢。是通过这个函数。直接将这个结果返回了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在实际的应用时。你可以通过API。或者抓取网页数据的方式呢。来获取真实的实时的数据。我分别测了一下这个20B和120B的模型。这个20B的模型。最开始呢。一直是没法正确的解答这个问题。后来我发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实是这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他对中文的编码呢比较敏感。后来调整一下这个中文的编码呢。就OK了但是这个120B的模型。对中文的编码就不敏感。一直呢是可以进行正确回答的。从这一点呢。就可以看出来。这个模型的参数量。对模型能力的影响。还真是挺大的。那第三个代码呢。就是基于这个OpenAI agent的一个实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">agent的实现呢。你可以简单的理解为。就是将这一坨的代码都给封装了起来。所以最终的代码看起来就简洁多了。那这个代码在两个模型上运行呢。都是没有任何问题的。最后呢。是我整理的一个非Serve的运行方式。那这个脚本呢。开始执行时呢。会有一个模型导入的一个过程。所以最开始。的执行。时间还是有些长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_339.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后我列了一下。这两个模型的一个简单的对比。关于这两个模型的能力呢。这个毋庸置疑。120B的模型。很明显是强过这个20B的模型。尤其是在针对这个中文的处理能力上。120B的模型的可用性呢。还是比较高的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是我关注的这个显存占用。在我这个实测当中呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个20B的模型。占用的显存呢是14个G。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而120B的模型呢。占用显存。大约是65个G。最后点呢。就是推理速度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">推理速度呢。20B的模型每秒呢。大约是155个TOKEN。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而120B的模型呢。每秒是141个token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我觉得这个结果。还是非常有意思的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是你可以看到。这个模型的参数量的对比。是120B和20B参数量相差呢是6倍的关系。但是它的推理速度。可以说差距是非常的小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那之所以这样呢。就是因为。它每次推理时。它的激活的参数量差距是非常小的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可以看到120B的模型呢。它是5.13B。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_401.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而20B呢是3.61B。这个120B的模型。它的这个显存占用呢。只需要65G左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">推理速度呢。还是非常的快的。所以还是非常适合。在本地进行部署运行的。本地部署相关的说明和代码呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家可以去星球。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">或者我们的b站小店中呢。去下载好。以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>GPT-OSS 本地部署实测，120B与20B如何做到同样快的？</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/vz8np0yiNvw/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=vz8np0yiNvw">https://www.youtube.com/watch?v=vz8np0yiNvw</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解 OpenAI 開源的 GPT-OSS 混合專家模型 (MoE) 的技術特性與設計理念。</li> <li>學習如何在本地端部署這些大型語言模型，並了解不同部署方式的適用情境。</li> <li>比較 20B 和 120B 兩種模型在能力、顯存佔用和推理速度上的實際表現，特別是其高效能的背後原因。</li> </ul> <hr /> <h1>摘要</h1> <h2>OpenAI GPT-OSS 模型概述與技術特點</h2> <p>影片開頭，作者介紹了 OpenAI 開源的 <strong>GPT-OSS</strong> 模型，並分享其在本地部署與測試這兩款模型的心得。首先，作者透過技術報告深入探討了這些模型的關鍵特性。這些開源模型具備出色的指令遵循、工具使用、基於 CoT (Chain of Thought) 的深度推理以及結構化輸出能力。然而，儘管模型能力全面且在排行榜上表現不錯，但 <strong>OpenAI 刻意將其能力限制在非「高能力」水平</strong>，以防止惡意濫用。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_0.jpg" /></p></p> <p>在模型參數分佈方面，<strong>MLP (多層感知器) 部分佔據了高達 90% 的參數</strong>，這正是 MoE 模型中「專家部分」的核心。為此，OpenAI 進一步對 MoE 部分進行了後訓練階段的 <strong>MXFP4 格式量化</strong>，將每個參數量化到 4.25 比特。這使得 120B 模型能在 80G 顯卡上運行，而 20B 模型僅需 16G 顯卡。作者也提到，120B 和 20B 模型分別擁有 128 和 32 個專家，但每個 TOKEN 處理時只會同時激活四個專家，這意味著儘管總參數量差距大，但實際激活的參數量差異不大。此外，該模型採用了與 Hugging Face tokenizer 不同的獨特 <strong>tokenizer</strong>。訓練時間方面，120B 模型需 210 萬小時 H100 GPU 時間，20B 模型約 20 萬小時，相當於使用 32 台各帶 8 張 H100 GPU 的機器訓練約一個月。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_30.jpg" /></p></p> <hr /> <h2>本地部署方式與實踐</h2> <p>影片接著介紹了多種本地部署 <strong>GPT-OSS</strong> 模型的方式。最簡便的是使用 <strong>Ollama</strong>，此外也可參考 <strong>unsloth</strong> 或 <strong>OpenAI 官方推薦的方式</strong>。作者主要採用 OpenAI 官方方式，其提供了兩種部署模式：一種是基於 <strong>vLLM Serve</strong> 的方式，適用於將模型部署為對話模型或進行 Demo 演示；另一種是<strong>非 Serve 模式</strong>，更適合進行大量的並行數據處理。這兩種方式都基於 <strong>vLLM</strong> 框架，因此安裝步驟大致相同。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_240.jpg" /></p></p> <hr /> <h2>模型性能測試與對比</h2> <p>作者準備了三個實例來測試模型。首先是讓模型撰寫一份關於特朗普的介紹：20B 模型的內容問題較多且中文表達奇怪，而 120B 模型則表現出色，明顯展現了兩者在能力上的巨大差距。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_360.jpg" /></p></p> <p>其次是測試模型呼叫外部工具的功能，例如查詢北京天氣。20B 模型起初無法正確解答，後經調整中文編碼才解決，這顯示其對中文編碼的敏感性；而 120B 模型則對中文編碼不敏感，一直能正確回答，再次印證了參數量對模型能力（特別是中文處理）的顯著影響。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_420.jpg" /></p></p> <p>第三個實例是基於 OpenAI agent 的實現，該實現將複雜的程式碼進行了封裝，最終程式碼更為簡潔。這部分測試中，兩個模型都能順利運行。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_8.jpg" /></p></p> <p>最後，作者提供了一份詳細的 20B 與 120B 模型對比：</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>1.  <strong>模型能力：</strong> 毫無疑問，120B 模型遠勝於 20B 模型，尤其在處理 <strong>中文</strong> 的可用性上，120B 表現更佳。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>2.  <strong>顯存佔用：</strong> 實測顯示，20B 模型佔用約 14GB 顯存，120B 模型約佔用 65GB 顯存。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_9.jpg" /></p></p> <p>3.  <strong>推理速度：</strong> 這是最令人驚訝的結果，20B 模型每秒約處理 155 個 TOKEN，而 120B 模型每秒約 141 個 TOKEN。儘管總參數量相差六倍，但 <strong>推理速度</strong> 的差距卻非常小。作者解釋，這是因為兩個模型在推理時實際 <strong>激活的參數量</strong> 差距不大（120B 約 5.13B，20B 約 3.61B）。因此，儘管 120B 模型顯存佔用較高，但其快速的推理速度使其非常適合在本地進行部署和運行。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_10.jpg" /></p></p> <p>影片結尾，作者提到相關的本地部署說明和程式碼可在特定平台下載。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_10.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好上周呢。OpenAI开源了GPT-OSS模型。这周呢我在本地部署。并测试了一下这两款模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就来为大家介绍一下这两款模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">来看一下如何在本地部署。以及。他们在本地部署时的推理速度的差距。到底有多大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先我们先通过他的技术报告呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">来看一下这个模型的一些特点。关于这个模型。下面几点呢。是需要我们了解的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个就是OpenAI的这款开源模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">支持的能力还是非常全的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">包括出色的指令遵循能力。工具使用能力。基于CoT的深度推理能力。以及结构化的输出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能力比较全面。刷榜性能呢。也不错但是呢。这个模型的能力。算不上强。根据OpenAI技术报告中的说明。即便是对这个120B的模型进行微调。这个模型的能力。也无法达到高能力。那他这里的高能力。并不是说这个模型刷榜的能力不行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是呢针对我们真实使用时。一些复杂场景中的一个处理的能力。即使你看他们这个刷榜的结果。他的这个数值。还是非常不错的。那么为什么模型达不到高能力呢。OpenAI的解释。是他们故意为之的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_66.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">原因呢就是防止。这个模型能力太强。会被恶意的使用。第三点就是模型参数的分布。不同参数量的模型。它到底是哪些部分。占用了这么大的参数量呢。这个呢我们可以通过这个表。就可以知道。也就是说MLP这部分。它占用了是90%的参数。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实这一部分呢就。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是我们常说的。这个MoE模型的专家部分。所对应的一些层。所以呢第四点就来了。OpenAI呢就进一步。对这个模型的MoE的部分。进行了量化处理。通过在后训练阶段呢。将MoE的参数。进行这个MXFP4格式的量化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是将每个参数量化到4.25比特。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那量化后呢。120B的模型可以在80G的显卡上运行。而20B的模型呢。只需要16G的显卡就可以运行了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第五点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是大家可以看一下。这个模型的专家数量。这个120B和20B的两款模型呢。分别的专家数量呢是128和32。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个TOKEN处理时呢。同时只会激活四个专家。所以说呢。你可以看到这个地方。这两款模型。虽然说它的参数量差距很大。但是它的激活的参数量呢。差别是不是很大的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_133.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第六点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个模型的tokenizer是比较特殊的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个它与我们常用的。这个基于huggingface tokenizer。是有些不同的。这个是它一个比较独特的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第七点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个模型训练所使用的时间。它这边的时间呢。是基于H100 GPU进行计算的。120B的模型呢是需要210万个小时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而20B的模型呢。大约是需要20万小时左右。那大约是一个什么样的计算量呢。假设你使用32台。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每台机器呢。带8张H100 GPU的机器进行训练的话呢。大约呢就需要一个月的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那关于第八点呢。就是这模型。关于模型安全。幻觉等评价相关的内容。还是做了很多介绍的对。这方面内容感兴趣的小伙伴呢。可以详细的阅读一下。这个技术报告的后面部分。好以上呢。就是这个技术报告当中呢。介绍的一些主要的内容。下面呢给大家介绍一下。如何在本地部署运行这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要使用这个模型。目前有多种方式。第一个呢。最简单的方式呢。就是你可以直接使用Ollama呢。来下载并运行它。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_196.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是你也可以参考这个unsloth提供的。这个本地部署的说明。来运行它。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个呢。就是参考OpenAI官方推荐的方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我这边呢。主要参考的是OpenAI官方的方式。来运行的这个模型。那这边的介绍呢。它提供的是两种方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是基于这个vLLM Serve的一种方式。另一种呢是一种非Serve的方式。那这两种方式有什么样的差别呢。第一个就是。如果你希望将这个模型部署后。作为对话模型使用。或者是呢。用于做一些Demo的演示。那么你可以使用这个基于Serve的方式。那如果呢。你希望做一些大量的数据处理。尤其是。进行并行的处理的话呢。这个时候。使用这个飞serve的方式呢。就会更方便一些。那这两种部署方式呢。它都是基于这个vLLM的。所以安装起来呢。基本上是一样的步骤。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">参考他这个代码呢。我准备了三个例子。第一个呢。是我让他写一个关于特朗普的介绍。这边呢是这个20B模型写出来的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里一看。就有很多的问题。很多的中文表达上。也是有些。奇怪但是你再看一下。这个120B模型写的这篇介绍。看起来呢。就非常的不错了。那第二个呢。就是我测试了一下。他调用外部工具的一个功能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_267.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这边给出的这个例子。它有一些问题。而这边呢。是参考了其他的代码。来重新写了一个完整的大语言模型。调用外部工具的代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个代码呢。其实也很简单。问题呢就是问一下北京的天气。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">数据获取呢。是通过这个函数。直接将这个结果返回了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在实际的应用时。你可以通过API。或者抓取网页数据的方式呢。来获取真实的实时的数据。我分别测了一下这个20B和120B的模型。这个20B的模型。最开始呢。一直是没法正确的解答这个问题。后来我发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实是这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他对中文的编码呢比较敏感。后来调整一下这个中文的编码呢。就OK了但是这个120B的模型。对中文的编码就不敏感。一直呢是可以进行正确回答的。从这一点呢。就可以看出来。这个模型的参数量。对模型能力的影响。还真是挺大的。那第三个代码呢。就是基于这个OpenAI agent的一个实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">agent的实现呢。你可以简单的理解为。就是将这一坨的代码都给封装了起来。所以最终的代码看起来就简洁多了。那这个代码在两个模型上运行呢。都是没有任何问题的。最后呢。是我整理的一个非Serve的运行方式。那这个脚本呢。开始执行时呢。会有一个模型导入的一个过程。所以最开始。的执行。时间还是有些长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_339.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后我列了一下。这两个模型的一个简单的对比。关于这两个模型的能力呢。这个毋庸置疑。120B的模型。很明显是强过这个20B的模型。尤其是在针对这个中文的处理能力上。120B的模型的可用性呢。还是比较高的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是我关注的这个显存占用。在我这个实测当中呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个20B的模型。占用的显存呢是14个G。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而120B的模型呢。占用显存。大约是65个G。最后点呢。就是推理速度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">推理速度呢。20B的模型每秒呢。大约是155个TOKEN。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而120B的模型呢。每秒是141个token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我觉得这个结果。还是非常有意思的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是你可以看到。这个模型的参数量的对比。是120B和20B参数量相差呢是6倍的关系。但是它的推理速度。可以说差距是非常的小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那之所以这样呢。就是因为。它每次推理时。它的激活的参数量差距是非常小的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可以看到120B的模型呢。它是5.13B。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/vz8np0yiNvw_401.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而20B呢是3.61B。这个120B的模型。它的这个显存占用呢。只需要65G左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">推理速度呢。还是非常的快的。所以还是非常适合。在本地进行部署运行的。本地部署相关的说明和代码呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家可以去星球。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">或者我们的b站小店中呢。去下载好。以上就是今天的分享。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
=============
<p>GPT-OSS 本地部署实测，120B与20B如何做到同样快的？</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/vz8np0yiNvw/hqdefault.jpg"/>
      <pubDate>2025-08-17T07:32:45.000Z</pubDate>
    </item><item>
      <title><![CDATA[AI技术周报20250810：GPT-5, OSS, Claude，Qwen3集中发布新模型！]]></title>
      <link>https://www.youtube.com/watch?v=yPVnPL5c0B0</link>
      <itunes:title><![CDATA[AI技术周报20250810：GPT-5, OSS, Claude，Qwen3集中发布新模型！]]></itunes:title>
      <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p><img src="https://img.youtube.com/vi/yPVnPL5c0B0/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=yPVnPL5c0B0">https://www.youtube.com/watch?v=yPVnPL5c0B0</a></p><h1>值得閱讀的理由</h1> <ul><li>掌握最新AI模型進展，包括OpenAI的GPT-5及其開源模型，以及Anthropic和Qwen3等主要玩家的新發布，助您緊跟AI前沿技術脈動。</li><li>深入了解當前AI市場的競爭格局，特別是OpenAI從封閉走向開源的策略轉變，以及MCP（Agent應用開發平台）如何推動Claude模型的使用與Anthropic的市場地位。</li><li>探索大模型訓練與推理技術的最新突破，包括GPO技術的改進、視覺大模型訓練的支援，以及一系列開源開發庫的更新，為技術開發者提供實用參考。</li></ul><hr /> <h1>摘要</h1> <p>這段影片中，作者介紹了本週AI開發領域的熱點內容。首先，本週的大模型新聞主要被<strong>OpenAI</strong>所主導。OpenAI發布了兩款開源模型，分別是<strong>GPT OSS 20B</strong>和<strong>120B</strong>，而備受期待的<strong>GPT-5</strong>也終於上線了。作者提到，OpenAI對GPT-5充滿信心，甚至直接將網站上的GPT-4o等舊模型替換成了GPT-5。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_25.jpg" /></p><hr /> <p>關於<strong>GPT-5</strong>的性能，作者個人體驗認為它確實比GPT-4o更好。然而，作者也指出，隨著人們對大模型的期望不斷升高，滿意度下降已是不可避免的趨勢。這就像2022年底ChatGPT剛出現時，大家滿意度極高，但隨著使用越來越多，人們的需求也變得越來越複雜。因此，<strong>GPT-5</strong>究竟是好是壞，不同的人可能有不同的看法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_18.jpg" /></p><hr /> <p>相較於<strong>GPT-5</strong>，作者個人認為OpenAI發布的兩款<strong>開源模型</strong>，尤其是120B版本，更值得期待。基於OpenAI多年累積的經驗和數據，這些開源模型的性能和穩定性預計會非常出色。這些模型的下載量也印證了這一點，例如在Ollama上已上升到第六位，在Hugging Face上，20B版本在最新的月下載量榜單中也已攀升至第47位，預計很快就能進入大模型類別的前三名。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_8.jpg" /></p><hr /> <p>影片中也探討了OpenAI為何再次轉向開源。作者認為，除了可能受到DeepSeek的刺激外，最主要的原因是<strong>MCP（Agent應用開發平台）</strong>的興起。MCP目前非常熱門，其給出的範例和說明文檔大多使用<strong>Anthropic</strong>的<strong>Claude模型</strong>，甚至推薦使用Claude Desktop。作者推斷，MCP的火爆將極大地推動Claude模型的應用，因為使用者在學習和使用MCP時，會自然地接觸並熟悉Claude，進而促進其使用量。事實上，在2025年上半年，Anthropic在面向企業的API應用收入上已經超越了OpenAI，這個時間點與MCP的火爆正好吻合。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_31.jpg" /></p><hr /> <p>此外，作者也強調，除了OpenAI，其他主要的AI公司也持續活躍。近期各個開發庫的更新日誌顯示，它們主要增加了對<strong>GPT-5</strong>和<strong>GPT OSS</strong>模型的支援。同時，OpenAI最大的競爭對手<strong>Anthropic</strong>也在本週發布了<strong>Claude Opus 4.1</strong>版本，其性能有顯著提升。另一家大廠<strong>Qwen3（通義千問）</strong>也發布了最新更新的模型，包括4B、30B、235B的Instruct和Thinking版本，以及30B、480B的Code版本，顯示開源模型領域的競爭日益激烈，Qwen3也突破了480B的規模。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_30.jpg" /></p><hr /> <p>在模型訓練和推理方面，相關開發庫的更新也相當頻繁。特別是針對<strong>GPO技術</strong>的訓練方法改進非常多，例如Hugging Face的TRL開發庫新增了sequence-level的GPO方法。<strong>GPO Trainer</strong>甚至開始支援視覺大模型的訓練。此外，清華大學也推出了一個名為<strong>Slime</strong>的強化學習訓練框架，都相當有意思。作者提到，相關的技術細節和學習資源非常豐富，並推薦有興趣的讀者自行查閱。影片最後，作者介紹了一些實用專案，例如DeepLearning.AI公開了一個使用<strong>Claude Code</strong>來構建Agent Coding Assistant的教學，以及一個結合<strong>Gradio</strong>和<strong>MCP</strong>來構建大模型驅動的AI購物助手的實踐案例。作者表示，學習內容非常豐富，未來也會分享更多感興趣的內容，並鼓勵觀眾在影片下方留言，提出希望了解的主題。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_249.jpg" /></p><hr />

<hr style="clear:both" />
=============
<p>AI技术周报20250810：GPT-5, OSS, Claude，Qwen3集中发布新模型！</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p><img src="https://img.youtube.com/vi/yPVnPL5c0B0/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=yPVnPL5c0B0">https://www.youtube.com/watch?v=yPVnPL5c0B0</a></p><h1>值得閱讀的理由</h1> <ul><li>掌握最新AI模型進展，包括OpenAI的GPT-5及其開源模型，以及Anthropic和Qwen3等主要玩家的新發布，助您緊跟AI前沿技術脈動。</li><li>深入了解當前AI市場的競爭格局，特別是OpenAI從封閉走向開源的策略轉變，以及MCP（Agent應用開發平台）如何推動Claude模型的使用與Anthropic的市場地位。</li><li>探索大模型訓練與推理技術的最新突破，包括GPO技術的改進、視覺大模型訓練的支援，以及一系列開源開發庫的更新，為技術開發者提供實用參考。</li></ul><hr /> <h1>摘要</h1> <p>這段影片中，作者介紹了本週AI開發領域的熱點內容。首先，本週的大模型新聞主要被<strong>OpenAI</strong>所主導。OpenAI發布了兩款開源模型，分別是<strong>GPT OSS 20B</strong>和<strong>120B</strong>，而備受期待的<strong>GPT-5</strong>也終於上線了。作者提到，OpenAI對GPT-5充滿信心，甚至直接將網站上的GPT-4o等舊模型替換成了GPT-5。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_25.jpg" /></p><hr /> <p>關於<strong>GPT-5</strong>的性能，作者個人體驗認為它確實比GPT-4o更好。然而，作者也指出，隨著人們對大模型的期望不斷升高，滿意度下降已是不可避免的趨勢。這就像2022年底ChatGPT剛出現時，大家滿意度極高，但隨著使用越來越多，人們的需求也變得越來越複雜。因此，<strong>GPT-5</strong>究竟是好是壞，不同的人可能有不同的看法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_18.jpg" /></p><hr /> <p>相較於<strong>GPT-5</strong>，作者個人認為OpenAI發布的兩款<strong>開源模型</strong>，尤其是120B版本，更值得期待。基於OpenAI多年累積的經驗和數據，這些開源模型的性能和穩定性預計會非常出色。這些模型的下載量也印證了這一點，例如在Ollama上已上升到第六位，在Hugging Face上，20B版本在最新的月下載量榜單中也已攀升至第47位，預計很快就能進入大模型類別的前三名。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_8.jpg" /></p><hr /> <p>影片中也探討了OpenAI為何再次轉向開源。作者認為，除了可能受到DeepSeek的刺激外，最主要的原因是<strong>MCP（Agent應用開發平台）</strong>的興起。MCP目前非常熱門，其給出的範例和說明文檔大多使用<strong>Anthropic</strong>的<strong>Claude模型</strong>，甚至推薦使用Claude Desktop。作者推斷，MCP的火爆將極大地推動Claude模型的應用，因為使用者在學習和使用MCP時，會自然地接觸並熟悉Claude，進而促進其使用量。事實上，在2025年上半年，Anthropic在面向企業的API應用收入上已經超越了OpenAI，這個時間點與MCP的火爆正好吻合。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_31.jpg" /></p><hr /> <p>此外，作者也強調，除了OpenAI，其他主要的AI公司也持續活躍。近期各個開發庫的更新日誌顯示，它們主要增加了對<strong>GPT-5</strong>和<strong>GPT OSS</strong>模型的支援。同時，OpenAI最大的競爭對手<strong>Anthropic</strong>也在本週發布了<strong>Claude Opus 4.1</strong>版本，其性能有顯著提升。另一家大廠<strong>Qwen3（通義千問）</strong>也發布了最新更新的模型，包括4B、30B、235B的Instruct和Thinking版本，以及30B、480B的Code版本，顯示開源模型領域的競爭日益激烈，Qwen3也突破了480B的規模。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_30.jpg" /></p><hr /> <p>在模型訓練和推理方面，相關開發庫的更新也相當頻繁。特別是針對<strong>GPO技術</strong>的訓練方法改進非常多，例如Hugging Face的TRL開發庫新增了sequence-level的GPO方法。<strong>GPO Trainer</strong>甚至開始支援視覺大模型的訓練。此外，清華大學也推出了一個名為<strong>Slime</strong>的強化學習訓練框架，都相當有意思。作者提到，相關的技術細節和學習資源非常豐富，並推薦有興趣的讀者自行查閱。影片最後，作者介紹了一些實用專案，例如DeepLearning.AI公開了一個使用<strong>Claude Code</strong>來構建Agent Coding Assistant的教學，以及一個結合<strong>Gradio</strong>和<strong>MCP</strong>來構建大模型驅動的AI購物助手的實踐案例。作者表示，學習內容非常豐富，未來也會分享更多感興趣的內容，並鼓勵觀眾在影片下方留言，提出希望了解的主題。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_249.jpg" /></p><hr />

<hr style="clear:both" />
=============
<p>AI技术周报20250810：GPT-5, OSS, Claude，Qwen3集中发布新模型！</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p><img src="https://img.youtube.com/vi/yPVnPL5c0B0/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=yPVnPL5c0B0">https://www.youtube.com/watch?v=yPVnPL5c0B0</a></p><h1>值得閱讀的理由</h1> <ul><li>掌握最新AI模型進展，包括OpenAI的GPT-5及其開源模型，以及Anthropic和Qwen3等主要玩家的新發布，助您緊跟AI前沿技術脈動。</li><li>深入了解當前AI市場的競爭格局，特別是OpenAI從封閉走向開源的策略轉變，以及MCP（Agent應用開發平台）如何推動Claude模型的使用與Anthropic的市場地位。</li><li>探索大模型訓練與推理技術的最新突破，包括GPO技術的改進、視覺大模型訓練的支援，以及一系列開源開發庫的更新，為技術開發者提供實用參考。</li></ul><hr /> <h1>摘要</h1> <p>這段影片中，作者介紹了本週AI開發領域的熱點內容。首先，本週的大模型新聞主要被<strong>OpenAI</strong>所主導。OpenAI發布了兩款開源模型，分別是<strong>GPT OSS 20B</strong>和<strong>120B</strong>，而備受期待的<strong>GPT-5</strong>也終於上線了。作者提到，OpenAI對GPT-5充滿信心，甚至直接將網站上的GPT-4o等舊模型替換成了GPT-5。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_25.jpg" /></p><hr /> <p>關於<strong>GPT-5</strong>的性能，作者個人體驗認為它確實比GPT-4o更好。然而，作者也指出，隨著人們對大模型的期望不斷升高，滿意度下降已是不可避免的趨勢。這就像2022年底ChatGPT剛出現時，大家滿意度極高，但隨著使用越來越多，人們的需求也變得越來越複雜。因此，<strong>GPT-5</strong>究竟是好是壞，不同的人可能有不同的看法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_18.jpg" /></p><hr /> <p>相較於<strong>GPT-5</strong>，作者個人認為OpenAI發布的兩款<strong>開源模型</strong>，尤其是120B版本，更值得期待。基於OpenAI多年累積的經驗和數據，這些開源模型的性能和穩定性預計會非常出色。這些模型的下載量也印證了這一點，例如在Ollama上已上升到第六位，在Hugging Face上，20B版本在最新的月下載量榜單中也已攀升至第47位，預計很快就能進入大模型類別的前三名。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_8.jpg" /></p><hr /> <p>影片中也探討了OpenAI為何再次轉向開源。作者認為，除了可能受到DeepSeek的刺激外，最主要的原因是<strong>MCP（Agent應用開發平台）</strong>的興起。MCP目前非常熱門，其給出的範例和說明文檔大多使用<strong>Anthropic</strong>的<strong>Claude模型</strong>，甚至推薦使用Claude Desktop。作者推斷，MCP的火爆將極大地推動Claude模型的應用，因為使用者在學習和使用MCP時，會自然地接觸並熟悉Claude，進而促進其使用量。事實上，在2025年上半年，Anthropic在面向企業的API應用收入上已經超越了OpenAI，這個時間點與MCP的火爆正好吻合。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_31.jpg" /></p><hr /> <p>此外，作者也強調，除了OpenAI，其他主要的AI公司也持續活躍。近期各個開發庫的更新日誌顯示，它們主要增加了對<strong>GPT-5</strong>和<strong>GPT OSS</strong>模型的支援。同時，OpenAI最大的競爭對手<strong>Anthropic</strong>也在本週發布了<strong>Claude Opus 4.1</strong>版本，其性能有顯著提升。另一家大廠<strong>Qwen3（通義千問）</strong>也發布了最新更新的模型，包括4B、30B、235B的Instruct和Thinking版本，以及30B、480B的Code版本，顯示開源模型領域的競爭日益激烈，Qwen3也突破了480B的規模。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_30.jpg" /></p><hr /> <p>在模型訓練和推理方面，相關開發庫的更新也相當頻繁。特別是針對<strong>GPO技術</strong>的訓練方法改進非常多，例如Hugging Face的TRL開發庫新增了sequence-level的GPO方法。<strong>GPO Trainer</strong>甚至開始支援視覺大模型的訓練。此外，清華大學也推出了一個名為<strong>Slime</strong>的強化學習訓練框架，都相當有意思。作者提到，相關的技術細節和學習資源非常豐富，並推薦有興趣的讀者自行查閱。影片最後，作者介紹了一些實用專案，例如DeepLearning.AI公開了一個使用<strong>Claude Code</strong>來構建Agent Coding Assistant的教學，以及一個結合<strong>Gradio</strong>和<strong>MCP</strong>來構建大模型驅動的AI購物助手的實踐案例。作者表示，學習內容非常豐富，未來也會分享更多感興趣的內容，並鼓勵觀眾在影片下方留言，提出希望了解的主題。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/yPVnPL5c0B0_249.jpg" /></p><hr />

<hr style="clear:both" />
=============
<p>AI技术周报20250810：GPT-5, OSS, Claude，Qwen3集中发布新模型！</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/yPVnPL5c0B0/hqdefault.jpg"/>
      <pubDate>2025-08-10T14:20:10.000Z</pubDate>
    </item><item>
      <title><![CDATA[AI技术周报20250727：JanusFlow下载暴涨，Voxtral来势汹汹！]]></title>
      <link>https://www.youtube.com/watch?v=wNqaGOC27P8</link>
      <itunes:title><![CDATA[AI技术周报20250727：JanusFlow下载暴涨，Voxtral来势汹汹！]]></itunes:title>
      <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p><img src="https://img.youtube.com/vi/wNqaGOC27P8/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=wNqaGOC27P8">https://www.youtube.com/watch?v=wNqaGOC27P8</a></p><h1>值得閱讀的理由</h1><ul><li>深入了解本週<strong>AI領域</strong>的最新進展，包括<strong>熱門模型</strong>的下載趨勢與新技術發布。</li><li>探討<strong>多模態AI模型</strong>的崛起及其在實現人工通用智慧（<strong>AGI</strong>）中的關鍵作用。</li><li>掌握<strong>開源開發庫</strong>的更新動態，以及值得關注的<strong>學習資源</strong>。</li></ul><hr /><h1>摘要</h1><h2>人工智慧週報概覽</h2><p>這集影片中，作者繼續為觀眾介紹本週<strong>AI開發相關的熱點內容</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_0.jpg" /></p><hr /><h2>JanusFlow下載量暴增</h2><p>作者首先檢視了<strong>Hugging Face</strong>上<strong>開源大模型</strong>的下載數據。本週在<strong>多模態輸入與輸出（Any-to-Any）類別</strong>中，有一個模型<strong>下載量大幅度上升</strong>。目前該類別排名第一的是熟悉的<strong>千問2.5 Omni</strong>。本週值得關注的模型是<strong>DeepSeek的Janus視覺大模型</strong>，其中<strong>JanusFlow</strong>模型在過去一週下載量突然暴漲，從上週的392次躍升至本週的19,937次。作者解釋說，<strong>JanusFlow</strong>是一個同時支援<strong>視覺理解與圖像生成</strong>的<strong>多模態大模型</strong>，其架構基於<strong>DeepSeek LLM 1.3B Base</strong>。Janus系列模型還包括Janus 1.3B、Janus Pro 1B和7B，主要區別在於<strong>圖像生成模塊</strong>。<strong>JanusFlow</strong>採用了<strong>Rectified Flow</strong>的生成方式，而Janus則採用了<strong>Next Token預測</strong>方式。作者指出，<strong>Janus Pro 7B</strong>的下載量緊隨<strong>千問2.5 Omni</strong>之後，目前排在第二位。<strong>JanusFlow</strong>下載量的突然大增，或許是因為它採用的<strong>圖像生成方式</strong>近期受到更多關注。透過這個模型，我們可以看到現在<strong>圖像理解與圖像生成</strong>都可以由單一模型完成。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_9.jpg" /></p><hr /><h2>Voxtral來勢洶洶</h2><p>作者接著介紹了<strong>Mistral AI</strong>發布的一款<strong>語音理解模型——Voxtral模型</strong>。它屬於<strong>音訊-文本-文本（Audio-Text-to-Text）類別</strong>，即模型接受音訊和文本輸入，輸出文本。這類模型可以處理語音識別、語音翻譯、語音理解以及語音情感分析等任務。該類別目前排名第一的是<strong>Altro Vox系列</strong>。<strong>Voxtral模型</strong>是<strong>Mistral AI</strong>上週剛剛發布的，共有三個版本：針對語音識別優化的<strong>Voxtral Mini 3B Transcribed</strong>，以及<strong>Voxtral Mini 3B</strong>和<strong>Voxtral Small 24B</strong>。根據他們的測試結果，在<strong>語音識別</strong>方面，<strong>Voxtral</strong>可以超越<strong>GPT-4o、Gemini 2.5 Pro、Bisper以及微軟的Phi-4模型</strong>。在<strong>語音理解</strong>等任務上表現也非常出色。作者建議對語音處理感興趣的朋友可以進一步關注。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_113.jpg" /></p><hr /><h2>熱門開發庫與新技術</h2><p>隨後，作者探討了熱門開發庫的最新動態。<strong>Transformers庫</strong>支援了<strong>Ernie 4.5</strong>和<strong>Ernie 4.5 MoE模型</strong>，這些是<strong>百度開源的模型</strong>。<strong>FastGPT</strong>新增支援了<strong>Gemini 2.5、Groq 4</strong>和<strong>Kimi模型</strong>。<strong>VLLM</strong>新增支援的模型更多，包括<strong>Llama 4模型</strong>，以及<strong>華為昇騰（Huawei Ascend）</strong>開發的<strong>XL-AI 4.0大模型</strong>，該模型於7月15日發布，有32B和1.2B兩個版本，其性能可超越同參數數量的<strong>千問3系列模型</strong>。此外，還有微軟的<strong>Phi-4 Mini Flash Reasoning深度推理模型</strong>、混元的<strong>V1 Dense模型</strong>、螞蟻金服的<strong>Ling MoE模型</strong>，以及一個<strong>Rerank模型</strong>。最後還有基於Llama 3微調的<strong>Memotron Nano VL8B V1視覺大模型</strong>，以及前面介紹過的<strong>Voxtral模型</strong>。關於新的技術，本週<strong>VLLM</strong>引入了一個新的<strong>「專家並行」（Expert Parallel）</strong>技術，稱為<strong>Elastic Expert Parallel</strong>，可翻譯為<strong>彈性專家並行</strong>。它旨在解決當前<strong>MOE模型</strong>中<strong>專家並行策略</strong>靜態配置的問題，使其能夠<strong>動態擴容和縮容專家並行的實例</strong>，從而在不中斷服務的情況下完成資源調整。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_176.jpg" /></p><hr /><h2>學習資料推薦</h2><p>作者接著分享了值得關注的學習資料。第一篇是<strong>LlamaIndex</strong>的一篇部落格文章，介紹了如何透過使用<strong>視覺大語言模型</strong>，結合<strong>多模態理解與結構化輸出</strong>的方法，更智能、更準確地解析<strong>PDF文檔</strong>，解決傳統<strong>OCR加規則</strong>解析複雜佈局（如表格和圖表）的局限。第二篇是<strong>Hugging Face</strong>的部落格文章，主要介紹了使用<strong>Diffusers</strong>和<strong>PEFT</strong>針對<strong>Flux模型</strong>進行<strong>LoRA推理優化</strong>的方法，結合了多種技術如<strong>Torch Compile、Flash Attention 3</strong>和<strong>8位量化</strong>，最終實現了<strong>圖像生成模型約兩倍的加速</strong>。最後一篇是<strong>Hugging Face</strong>部落格中介紹的一個新的基準測試數據集<strong>TimeScope</strong>，它是一個<strong>開源的影片理解評測數據集</strong>，可用於測試<strong>視覺大模型</strong>處理一分鐘到八小時的<strong>長影片理解能力</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_293.jpg" /></p><hr /><h2>多模態AI的崛起</h2><p>作者總結道，從上述內容可以看出，近期<strong>AI領域</strong>有一個新趨勢，就是都在加強發展<strong>多模態大模型</strong>。從模型類別來看，它可以包括<strong>音訊-文本-文本、圖像-文本-文本、音訊-音訊、影片-文本-文本</strong>，甚至<strong>Any-to-Any</strong>。除了這些開源模型，像Google的<strong>Gemini</strong>本身就是<strong>原生態的多模態大模型</strong>，而傳說中即將發布的<strong>GPT-5</strong>據說也是一個<strong>多模態大模型</strong>。現在<strong>多模態大模型</strong>的發展越來越受到重視。作者解釋了原因：首先，基於文本的<strong>LLM發展</strong>已經進入瓶頸。下一步要實現<strong>AGI（人工通用智慧）</strong>，讓AI完成更多、更複雜場景中的任務，<strong>多模態大模型</strong>是不可或缺的一個非常重要的關鍵點。透過進一步發展<strong>多模態大模型</strong>，讓模型可以同時使用<strong>文本、語音、視覺</strong>等不同模態的數據進行訓練，這樣才有可能完成人類擅長的複雜場景任務。如果真的能做到這一點，人類可能離<strong>AGI</strong>真的就不遠了。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_375.jpg" /></p><hr /><h2>AGI的未來</h2><p>最後，作者向觀眾提問：<strong>AGI</strong>到底什麼時候才能來？是五年還是十年？還是說它永遠不能實現？他鼓勵觀眾在評論區留言，說出自己對<strong>AGI</strong>的看法。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_451.jpg" /></p><hr />

<hr style="clear:both" />
=============
<p>AI技术周报20250727：JanusFlow下载暴涨，Voxtral来势汹汹！</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p><img src="https://img.youtube.com/vi/wNqaGOC27P8/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=wNqaGOC27P8">https://www.youtube.com/watch?v=wNqaGOC27P8</a></p><h1>值得閱讀的理由</h1><ul><li>深入了解本週<strong>AI領域</strong>的最新進展，包括<strong>熱門模型</strong>的下載趨勢與新技術發布。</li><li>探討<strong>多模態AI模型</strong>的崛起及其在實現人工通用智慧（<strong>AGI</strong>）中的關鍵作用。</li><li>掌握<strong>開源開發庫</strong>的更新動態，以及值得關注的<strong>學習資源</strong>。</li></ul><hr /><h1>摘要</h1><h2>人工智慧週報概覽</h2><p>這集影片中，作者繼續為觀眾介紹本週<strong>AI開發相關的熱點內容</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_0.jpg" /></p><hr /><h2>JanusFlow下載量暴增</h2><p>作者首先檢視了<strong>Hugging Face</strong>上<strong>開源大模型</strong>的下載數據。本週在<strong>多模態輸入與輸出（Any-to-Any）類別</strong>中，有一個模型<strong>下載量大幅度上升</strong>。目前該類別排名第一的是熟悉的<strong>千問2.5 Omni</strong>。本週值得關注的模型是<strong>DeepSeek的Janus視覺大模型</strong>，其中<strong>JanusFlow</strong>模型在過去一週下載量突然暴漲，從上週的392次躍升至本週的19,937次。作者解釋說，<strong>JanusFlow</strong>是一個同時支援<strong>視覺理解與圖像生成</strong>的<strong>多模態大模型</strong>，其架構基於<strong>DeepSeek LLM 1.3B Base</strong>。Janus系列模型還包括Janus 1.3B、Janus Pro 1B和7B，主要區別在於<strong>圖像生成模塊</strong>。<strong>JanusFlow</strong>採用了<strong>Rectified Flow</strong>的生成方式，而Janus則採用了<strong>Next Token預測</strong>方式。作者指出，<strong>Janus Pro 7B</strong>的下載量緊隨<strong>千問2.5 Omni</strong>之後，目前排在第二位。<strong>JanusFlow</strong>下載量的突然大增，或許是因為它採用的<strong>圖像生成方式</strong>近期受到更多關注。透過這個模型，我們可以看到現在<strong>圖像理解與圖像生成</strong>都可以由單一模型完成。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_9.jpg" /></p><hr /><h2>Voxtral來勢洶洶</h2><p>作者接著介紹了<strong>Mistral AI</strong>發布的一款<strong>語音理解模型——Voxtral模型</strong>。它屬於<strong>音訊-文本-文本（Audio-Text-to-Text）類別</strong>，即模型接受音訊和文本輸入，輸出文本。這類模型可以處理語音識別、語音翻譯、語音理解以及語音情感分析等任務。該類別目前排名第一的是<strong>Altro Vox系列</strong>。<strong>Voxtral模型</strong>是<strong>Mistral AI</strong>上週剛剛發布的，共有三個版本：針對語音識別優化的<strong>Voxtral Mini 3B Transcribed</strong>，以及<strong>Voxtral Mini 3B</strong>和<strong>Voxtral Small 24B</strong>。根據他們的測試結果，在<strong>語音識別</strong>方面，<strong>Voxtral</strong>可以超越<strong>GPT-4o、Gemini 2.5 Pro、Bisper以及微軟的Phi-4模型</strong>。在<strong>語音理解</strong>等任務上表現也非常出色。作者建議對語音處理感興趣的朋友可以進一步關注。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_113.jpg" /></p><hr /><h2>熱門開發庫與新技術</h2><p>隨後，作者探討了熱門開發庫的最新動態。<strong>Transformers庫</strong>支援了<strong>Ernie 4.5</strong>和<strong>Ernie 4.5 MoE模型</strong>，這些是<strong>百度開源的模型</strong>。<strong>FastGPT</strong>新增支援了<strong>Gemini 2.5、Groq 4</strong>和<strong>Kimi模型</strong>。<strong>VLLM</strong>新增支援的模型更多，包括<strong>Llama 4模型</strong>，以及<strong>華為昇騰（Huawei Ascend）</strong>開發的<strong>XL-AI 4.0大模型</strong>，該模型於7月15日發布，有32B和1.2B兩個版本，其性能可超越同參數數量的<strong>千問3系列模型</strong>。此外，還有微軟的<strong>Phi-4 Mini Flash Reasoning深度推理模型</strong>、混元的<strong>V1 Dense模型</strong>、螞蟻金服的<strong>Ling MoE模型</strong>，以及一個<strong>Rerank模型</strong>。最後還有基於Llama 3微調的<strong>Memotron Nano VL8B V1視覺大模型</strong>，以及前面介紹過的<strong>Voxtral模型</strong>。關於新的技術，本週<strong>VLLM</strong>引入了一個新的<strong>「專家並行」（Expert Parallel）</strong>技術，稱為<strong>Elastic Expert Parallel</strong>，可翻譯為<strong>彈性專家並行</strong>。它旨在解決當前<strong>MOE模型</strong>中<strong>專家並行策略</strong>靜態配置的問題，使其能夠<strong>動態擴容和縮容專家並行的實例</strong>，從而在不中斷服務的情況下完成資源調整。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_176.jpg" /></p><hr /><h2>學習資料推薦</h2><p>作者接著分享了值得關注的學習資料。第一篇是<strong>LlamaIndex</strong>的一篇部落格文章，介紹了如何透過使用<strong>視覺大語言模型</strong>，結合<strong>多模態理解與結構化輸出</strong>的方法，更智能、更準確地解析<strong>PDF文檔</strong>，解決傳統<strong>OCR加規則</strong>解析複雜佈局（如表格和圖表）的局限。第二篇是<strong>Hugging Face</strong>的部落格文章，主要介紹了使用<strong>Diffusers</strong>和<strong>PEFT</strong>針對<strong>Flux模型</strong>進行<strong>LoRA推理優化</strong>的方法，結合了多種技術如<strong>Torch Compile、Flash Attention 3</strong>和<strong>8位量化</strong>，最終實現了<strong>圖像生成模型約兩倍的加速</strong>。最後一篇是<strong>Hugging Face</strong>部落格中介紹的一個新的基準測試數據集<strong>TimeScope</strong>，它是一個<strong>開源的影片理解評測數據集</strong>，可用於測試<strong>視覺大模型</strong>處理一分鐘到八小時的<strong>長影片理解能力</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_293.jpg" /></p><hr /><h2>多模態AI的崛起</h2><p>作者總結道，從上述內容可以看出，近期<strong>AI領域</strong>有一個新趨勢，就是都在加強發展<strong>多模態大模型</strong>。從模型類別來看，它可以包括<strong>音訊-文本-文本、圖像-文本-文本、音訊-音訊、影片-文本-文本</strong>，甚至<strong>Any-to-Any</strong>。除了這些開源模型，像Google的<strong>Gemini</strong>本身就是<strong>原生態的多模態大模型</strong>，而傳說中即將發布的<strong>GPT-5</strong>據說也是一個<strong>多模態大模型</strong>。現在<strong>多模態大模型</strong>的發展越來越受到重視。作者解釋了原因：首先，基於文本的<strong>LLM發展</strong>已經進入瓶頸。下一步要實現<strong>AGI（人工通用智慧）</strong>，讓AI完成更多、更複雜場景中的任務，<strong>多模態大模型</strong>是不可或缺的一個非常重要的關鍵點。透過進一步發展<strong>多模態大模型</strong>，讓模型可以同時使用<strong>文本、語音、視覺</strong>等不同模態的數據進行訓練，這樣才有可能完成人類擅長的複雜場景任務。如果真的能做到這一點，人類可能離<strong>AGI</strong>真的就不遠了。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_375.jpg" /></p><hr /><h2>AGI的未來</h2><p>最後，作者向觀眾提問：<strong>AGI</strong>到底什麼時候才能來？是五年還是十年？還是說它永遠不能實現？他鼓勵觀眾在評論區留言，說出自己對<strong>AGI</strong>的看法。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_451.jpg" /></p><hr />

<hr style="clear:both" />
=============
<p>AI技术周报20250727：JanusFlow下载暴涨，Voxtral来势汹汹！</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p><img src="https://img.youtube.com/vi/wNqaGOC27P8/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=wNqaGOC27P8">https://www.youtube.com/watch?v=wNqaGOC27P8</a></p><h1>值得閱讀的理由</h1><ul><li>深入了解本週<strong>AI領域</strong>的最新進展，包括<strong>熱門模型</strong>的下載趨勢與新技術發布。</li><li>探討<strong>多模態AI模型</strong>的崛起及其在實現人工通用智慧（<strong>AGI</strong>）中的關鍵作用。</li><li>掌握<strong>開源開發庫</strong>的更新動態，以及值得關注的<strong>學習資源</strong>。</li></ul><hr /><h1>摘要</h1><h2>人工智慧週報概覽</h2><p>這集影片中，作者繼續為觀眾介紹本週<strong>AI開發相關的熱點內容</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_0.jpg" /></p><hr /><h2>JanusFlow下載量暴增</h2><p>作者首先檢視了<strong>Hugging Face</strong>上<strong>開源大模型</strong>的下載數據。本週在<strong>多模態輸入與輸出（Any-to-Any）類別</strong>中，有一個模型<strong>下載量大幅度上升</strong>。目前該類別排名第一的是熟悉的<strong>千問2.5 Omni</strong>。本週值得關注的模型是<strong>DeepSeek的Janus視覺大模型</strong>，其中<strong>JanusFlow</strong>模型在過去一週下載量突然暴漲，從上週的392次躍升至本週的19,937次。作者解釋說，<strong>JanusFlow</strong>是一個同時支援<strong>視覺理解與圖像生成</strong>的<strong>多模態大模型</strong>，其架構基於<strong>DeepSeek LLM 1.3B Base</strong>。Janus系列模型還包括Janus 1.3B、Janus Pro 1B和7B，主要區別在於<strong>圖像生成模塊</strong>。<strong>JanusFlow</strong>採用了<strong>Rectified Flow</strong>的生成方式，而Janus則採用了<strong>Next Token預測</strong>方式。作者指出，<strong>Janus Pro 7B</strong>的下載量緊隨<strong>千問2.5 Omni</strong>之後，目前排在第二位。<strong>JanusFlow</strong>下載量的突然大增，或許是因為它採用的<strong>圖像生成方式</strong>近期受到更多關注。透過這個模型，我們可以看到現在<strong>圖像理解與圖像生成</strong>都可以由單一模型完成。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_9.jpg" /></p><hr /><h2>Voxtral來勢洶洶</h2><p>作者接著介紹了<strong>Mistral AI</strong>發布的一款<strong>語音理解模型——Voxtral模型</strong>。它屬於<strong>音訊-文本-文本（Audio-Text-to-Text）類別</strong>，即模型接受音訊和文本輸入，輸出文本。這類模型可以處理語音識別、語音翻譯、語音理解以及語音情感分析等任務。該類別目前排名第一的是<strong>Altro Vox系列</strong>。<strong>Voxtral模型</strong>是<strong>Mistral AI</strong>上週剛剛發布的，共有三個版本：針對語音識別優化的<strong>Voxtral Mini 3B Transcribed</strong>，以及<strong>Voxtral Mini 3B</strong>和<strong>Voxtral Small 24B</strong>。根據他們的測試結果，在<strong>語音識別</strong>方面，<strong>Voxtral</strong>可以超越<strong>GPT-4o、Gemini 2.5 Pro、Bisper以及微軟的Phi-4模型</strong>。在<strong>語音理解</strong>等任務上表現也非常出色。作者建議對語音處理感興趣的朋友可以進一步關注。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_113.jpg" /></p><hr /><h2>熱門開發庫與新技術</h2><p>隨後，作者探討了熱門開發庫的最新動態。<strong>Transformers庫</strong>支援了<strong>Ernie 4.5</strong>和<strong>Ernie 4.5 MoE模型</strong>，這些是<strong>百度開源的模型</strong>。<strong>FastGPT</strong>新增支援了<strong>Gemini 2.5、Groq 4</strong>和<strong>Kimi模型</strong>。<strong>VLLM</strong>新增支援的模型更多，包括<strong>Llama 4模型</strong>，以及<strong>華為昇騰（Huawei Ascend）</strong>開發的<strong>XL-AI 4.0大模型</strong>，該模型於7月15日發布，有32B和1.2B兩個版本，其性能可超越同參數數量的<strong>千問3系列模型</strong>。此外，還有微軟的<strong>Phi-4 Mini Flash Reasoning深度推理模型</strong>、混元的<strong>V1 Dense模型</strong>、螞蟻金服的<strong>Ling MoE模型</strong>，以及一個<strong>Rerank模型</strong>。最後還有基於Llama 3微調的<strong>Memotron Nano VL8B V1視覺大模型</strong>，以及前面介紹過的<strong>Voxtral模型</strong>。關於新的技術，本週<strong>VLLM</strong>引入了一個新的<strong>「專家並行」（Expert Parallel）</strong>技術，稱為<strong>Elastic Expert Parallel</strong>，可翻譯為<strong>彈性專家並行</strong>。它旨在解決當前<strong>MOE模型</strong>中<strong>專家並行策略</strong>靜態配置的問題，使其能夠<strong>動態擴容和縮容專家並行的實例</strong>，從而在不中斷服務的情況下完成資源調整。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_176.jpg" /></p><hr /><h2>學習資料推薦</h2><p>作者接著分享了值得關注的學習資料。第一篇是<strong>LlamaIndex</strong>的一篇部落格文章，介紹了如何透過使用<strong>視覺大語言模型</strong>，結合<strong>多模態理解與結構化輸出</strong>的方法，更智能、更準確地解析<strong>PDF文檔</strong>，解決傳統<strong>OCR加規則</strong>解析複雜佈局（如表格和圖表）的局限。第二篇是<strong>Hugging Face</strong>的部落格文章，主要介紹了使用<strong>Diffusers</strong>和<strong>PEFT</strong>針對<strong>Flux模型</strong>進行<strong>LoRA推理優化</strong>的方法，結合了多種技術如<strong>Torch Compile、Flash Attention 3</strong>和<strong>8位量化</strong>，最終實現了<strong>圖像生成模型約兩倍的加速</strong>。最後一篇是<strong>Hugging Face</strong>部落格中介紹的一個新的基準測試數據集<strong>TimeScope</strong>，它是一個<strong>開源的影片理解評測數據集</strong>，可用於測試<strong>視覺大模型</strong>處理一分鐘到八小時的<strong>長影片理解能力</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_293.jpg" /></p><hr /><h2>多模態AI的崛起</h2><p>作者總結道，從上述內容可以看出，近期<strong>AI領域</strong>有一個新趨勢，就是都在加強發展<strong>多模態大模型</strong>。從模型類別來看，它可以包括<strong>音訊-文本-文本、圖像-文本-文本、音訊-音訊、影片-文本-文本</strong>，甚至<strong>Any-to-Any</strong>。除了這些開源模型，像Google的<strong>Gemini</strong>本身就是<strong>原生態的多模態大模型</strong>，而傳說中即將發布的<strong>GPT-5</strong>據說也是一個<strong>多模態大模型</strong>。現在<strong>多模態大模型</strong>的發展越來越受到重視。作者解釋了原因：首先，基於文本的<strong>LLM發展</strong>已經進入瓶頸。下一步要實現<strong>AGI（人工通用智慧）</strong>，讓AI完成更多、更複雜場景中的任務，<strong>多模態大模型</strong>是不可或缺的一個非常重要的關鍵點。透過進一步發展<strong>多模態大模型</strong>，讓模型可以同時使用<strong>文本、語音、視覺</strong>等不同模態的數據進行訓練，這樣才有可能完成人類擅長的複雜場景任務。如果真的能做到這一點，人類可能離<strong>AGI</strong>真的就不遠了。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_375.jpg" /></p><hr /><h2>AGI的未來</h2><p>最後，作者向觀眾提問：<strong>AGI</strong>到底什麼時候才能來？是五年還是十年？還是說它永遠不能實現？他鼓勵觀眾在評論區留言，說出自己對<strong>AGI</strong>的看法。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Weekly-202503/file-cache/wNqaGOC27P8_451.jpg" /></p><hr />

<hr style="clear:both" />
=============
<p>AI技术周报20250727：JanusFlow下载暴涨，Voxtral来势汹汹！</p><p></p><p><a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a></p><p>AI开发者-就爱瞎鼓捣[YT+]</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/wNqaGOC27P8/hqdefault.jpg"/>
      <pubDate>2025-07-26T15:22:21.000Z</pubDate>
    </item></channel>
</rss>